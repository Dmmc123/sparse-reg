[{"text": "[D] Simple Questions Thread. Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!", "score": 11}, {"text": "Reminder: Use the report button and read the rules!. ", "score": 9}, {"text": "[D] The best way to train an LLM on company data. Hey guys, I want to train any LLM on my company\u2019s data we have stored in Azure and Snowflake  \nIt\u2019s all in tabular form, and I was wondering how can I train an LLM on the data, and be able to ask it questions about it. No computations required from the model, but at least be able to tell answer questions such as: What was Apple\u2019s return compared to it\u2019s sector last month ( we have financial data)\n\n\\- is it possible to train an LLM to understand tabluar data\n\n\\- is it possible to train it on Snowflake/Azure \n\nAny help or links would be appreciated!", "score": 51}, {"text": "[D] Alternatives to fb Hydra?. I have been trying to find a nice tech stack I like for designing and running machine learning models, and currently I'm trying out mlflow, hydra, and optuna.\n\nHowever, hydra seems to have several limitations that are really annoying and are making me reconsider my choice. Most problematic is the inability to group parameters together in a multirun. Hydra only supports trying all combinations of parameters, as described in https://github.com/facebookresearch/hydra/issues/1258, which does not seem to be a priority for hydra. Furthermore, hydras optuna optimizer implementation does not allow for early pruning of bad runs, which while not a deal breaker is definitely a nice to have feature.\n\nWhat I do like about hydra is their ability to combine config yaml, using defaults. So does anyone have any good alternatives or suggestions for how to fix this or what to switch to?", "score": 27}, {"text": "[R] You Only Segment Once: Towards Real-Time Panoptic Segmentation [CVPR 2023]. Happy to introduce our latest work on Panoptic Segmentation: YOSO. And to the best of our knowledge, YOSO is **the first read-time panoptic segmentation framework that delivers competitive performance** compared to the state-of-the-art models.\n\nThe code is available here: [https://github.com/hujiecpp/YOSO](https://github.com/hujiecpp/YOSO)\n\nSpecifically, YOSO achieves **46.4 PQ, 45.5 FPS on COCO**; **52.5 PQ, 22.6 FPS on Cityscapes**; **38.0 PQ, 35.4 FPS on ADE20k** and **34.1 PQ 7.1 FPS on Mapillary Vistas**.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5bmcl1n6bmqa1.png?width=1362&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2f321fa31d8c3de5f53dbd61905a6448e9cb0a24\n\nhttps://preview.redd.it/o4pkvri7bmqa1.png?width=674&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fb609576c7ed615c3b0cb38a3db243eebe95e8e1\n\nhttps://preview.redd.it/ufdaa028bmqa1.png?width=681&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ecbceab2cb70ed8d63e4b37e7caa8068e63d367e\n\nhttps://preview.redd.it/y2t2upj8bmqa1.png?width=603&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4830fdb9817081466e2efad184ceb8c12b4b4f60\n\nhttps://preview.redd.it/3wiyep09bmqa1.png?width=597&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fd8fe26ff7112c3135e7774636a27de732b5893c\n\nhttps://preview.redd.it/g1lhwce9bmqa1.png?width=595&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ad7550b09ffc7c2f2eb18fe0178d5b4a43ddc4e7\n\n&amp;#x200B;", "score": 55}, {"text": "[R] The Debate Over Understanding in AI\u2019s Large Language Models. ", "score": 6}, {"text": "[P] An app using Story2vec approach to match people who are going through similar things. Hi all,  \nWe have developed an anonymous group chat app, namely Epicurus, which matches people with others who are going through similar things.  \nThe app uses a matching model which is very similar to this solution([https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/394812](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/394812)). Actually, the solution owner is our co-founder who got the first place.  \nIts AI engine matches users based on their stories. We believe people are looking for some empathy when they are in tough situations, and we may help them to support each other.  \nIf you could give a try and give your precious feedback, we would be grateful.  \nApp website: [https://epicurusapp.com/](https://epicurusapp.com/)\n\n[Images from the app](https://preview.redd.it/rjvtjwft8qqa1.png?width=1984&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b8107c968f3286eb3c7162addc1bf0b15fd37abf)", "score": 4}, {"text": "[D] Summer School on Systems Vision Science in Tuebingen, Germany, application deadline this Friday. Systems vision science combines computational, behavioral, and neuroscience methods to discover functions and algorithms for vision in various brain regions and their implementations in neural circuits. Should be interesting for computer vision/machine learning researchers interested in learning about biological vision", "score": 7}, {"text": "[Discussion] IsItBS: asking GPT to reflect x times will create a feedback loop that causes it to scrutinize itself x times?. I've seen posts claiming this. There is a paper saying that a scrutiny feedback loops can improve the performance of GPT-4 by 30%. I've experimented with feedback loops using the API and don't doubt that this can or, in future may be able to, produce emergent behaviour. I'm no expert but my surface-level understanding of transformers is that they would not create feedback loops just from prompting and would merely just respond as if they were.\n\nIf it were true, it would have significant economical implications since creating the feedback loop separately multiplies the price each loop.", "score": 3}, {"text": "[P] Imaginary programming: implementation-free TypeScript functions for GPT-powered web development. imaginary.dev is a project I built to allow web developers to use GPT to easily add AI features to their existing web user interfaces. All a developer does is declares a function prototype in TypeScript with a good comment saying what the function should do, and then they can call the function from other TypeScript and JavaScript code, even though they've never implemented the function in question. It looks something like:\n\n\n/**   \n * This function takes in a blog post text and returns at least 5 good titles for the blog post.  \n * The titles should be snappy and interesting and entice people to click on the blog post.  \n *  \n * @param blogPostText - string with the blog post text  \n * @returns an array of at least 5 good, enticing titles for the blog post.  \n *  \n * @imaginary  \n */\ndeclare function titleForBlogPost(blogPostText: string): Promise&lt;Array&lt;string&gt;&gt;;\n\n\nUnder the covers, we've written a TypeScript and Babel plugin that replaces these \"imaginary function\" declarations with *runtime* calls to GPT asking GPT what the theoretical function would return for a particular set of inputs. So it's not using GPT to write code (like CoPilot or Ghostwriter); it's using GPT to act as the runtime. This gives you freedom to implement things that you could never do in traditional programming: classification, extraction of structured information out of human language, translation, spell checking, creative generation, etc.\n\nHere's a screencast where I show off adding intelligent features to a simple blog post web app: https://www.loom.com/share/b367f4863fe843998270121131ae04d9\n\nLet me know what you think. Is this useful? Is this something you think you'd enjoy using? Is this a good direction to take web development? Happy to hear any and all feedback!", "score": 1}, {"text": "[R] AI-Virology Integration. Hello everyone,\n\nI am currently working on a research paper that explores the integration of AI-aided immune tweening with permafrost immunity. Permafrost melting has released ancient and novel pathogens that we have no cures for, posing a significant threat to public health.\n\nAs a way of identifying these viruses, we have discovered that AI can perform fractal analysis called fractalomics, which helps us understand their shapes. However, I am seeking more specific AI-related topics to look into and any related papers or researchers.\n\nAdditionally, any related ideas of integrating AI with microscopic biology and predictions of patterns would be helpful in understanding the impact of permafrost melting on public health.\n\nIf anyone has any insights or knowledge on the use of AI in identifying and characterizing immuno-regulatory pathway sequences or related topics, please feel free to share. Any information related to permafrost immunity and its impact on immune surveillance, oral pathology, and public health would also be appreciated.\n\nThank you for your help!", "score": 13}, {"text": "[D] Improvements/alternatives to U-net for medical images segmentation?. Hello,\n\nI am working on a project in which I'm detecting cavities in X-rays.\n\nThe dataset I have is pretty limited (\\~100 images). Each X-ray has a black and white mask that shows where in the image are the cavities.\n\nI'm trying to improve my results.\n\nWhat I've tried so far:\n\n1. different loss functions: BCE, dice loss, bce+dice, tversky loss, focal tversky loss\n2. modifying the images' gamma to make the cavities more visible\n3. trying out different U-Nets: U-net, V-net, U-net++, UNET 3+, Attention U-net, R2U-net, ResUnet-a, U\\^2-Net, TransUNET, and Swin-UNET\n\nNone of the new U-nets that I've tried improved the results. Probably because they are more suited for a larger dataset.\n\nI'm now looking for other things to try to improve my results. Currently my network is detecting cavities, but it has trouble with the smaller ones.", "score": 0}, {"text": "[N] OpenAI may have benchmarked GPT-4\u2019s coding ability on it\u2019s own training data. [GPT-4 and professional benchmarks: the wrong answer to the wrong question](https://aisnakeoil.substack.com/p/gpt-4-and-professional-benchmarks)\n\n*OpenAI may have tested on the training data. Besides, human benchmarks are meaningless for bots.*\n\n **Problem 1: training data contamination**\n\nTo benchmark GPT-4\u2019s coding ability, OpenAI evaluated it on problems from Codeforces, a website that hosts coding competitions. Surprisingly, Horace He pointed out that GPT-4 solved 10/10 pre-2021 problems and 0/10 recent problems in the easy category. The training data cutoff for GPT-4 is September 2021. This strongly suggests that the model is able to memorize solutions from its training set \u2014 or at least partly memorize them, enough that it can fill in what it can\u2019t recall.\n\nAs further evidence for this hypothesis, we tested it on Codeforces problems from different times in 2021. We found that it could regularly solve problems in the easy category before September 5, but none of the problems after September 12.\n\nIn fact, we can definitively show that it has memorized problems in its training set: when prompted with the title of a Codeforces problem, GPT-4 includes a link to the exact contest where the problem appears (and the round number is almost correct: it is off by one). Note that GPT-4 cannot access the Internet, so memorization is the only explanation.", "score": 875}, {"text": "[P] Kangas 2.0: EDA for Computer Vision Datasets. Project Link: [https://github.com/comet-ml/kangas](https://github.com/comet-ml/kangas)\n\n5 months ago, I shared the initial release of Kangas, a new open source EDA tool that my colleagues and I were working on, here in r/MachineLearning. After collecting feedback from some community members here, and from various other Kangas users, we've finally released version 2.0!\n\nKangas is a tool for viewing and exploring large tables of multimedia data. It allows you to ingest large tables of data\u2014from dataframes, csv's, or other sources\u2014via Kangas' Python library, and construct a data structure we call a DataGrid. From the DataGrid object, you can perform a variety of queries and operations, including rendering the Kangas UI by running `DataGrid.show()`\n\n&amp;#x200B;\n\nhttps://i.redd.it/fj3gvlbo5jqa1.gif\n\nWe've focused on a handful of features with release 2.0, including:\n\n* **Built-in CV support.** Kangas has out-of-the-box support for a variety of image metadata, including bounding boxes, masks, and annotations. In the above GIF, you can see some of the built-in visualizations that the Kangas UI provides without any extra setup.\n* **Scalability**. DataGrids are actually SQLite databases under the hood, served by a Flask server and rendered via React Server Components using Next.js. As a result, Kangas isn't bound by the same memory constraints as other libraries, and can render large quantities of data quickly regardless of environment.\n* **Interoperability**. Kangas can run as a standalone application, within a Jupyter notebook, or can be deployed on a server (as we've done at [https://kangas.comet.com](https://kangas.comet.com)). Additionally, Kangas has built-in integrations with HuggingFace and Comet to allow you to easily import and export datasets. In the future, we are planning on a more robust set of default integrations.\n* **Customizability**. You can create custom filters using a simple Python syntax, resize and reorder columns in whatever order you want, and perform complex querying logic directly from the Kangas UI. In the near future, we also plan to support completely custom visualizations for user-defined datatypes.\n\nWe're incredibly grateful to the community here for all the feedback and support you've offered so far. It's been very helpful to us in setting our roadmap, and motivating our continued work. If you're curious about Kangas, please take it for a spin by running any of the [Colab notebooks linked in the project README](https://github.com/comet-ml/kangas), or by visiting [https://kangas.comet.com](https://kangas.comet.com), where we've deployed a demo app.\n\nIf you have any questions or feedback, I'd love to hear it!  And thank you again for all your help so far.", "score": 81}, {"text": "[D] What would be the best way to build a catalogue of faces based on a bunch of videos?. Hi, I have a bunch of videos (about 1000) that contain various people, about 300 people.\n\nI want to build an app that allows me to build a catalogue of the people that appear in every video, based on their faces, so that i can just ask, \"In which videos does x person appear?\" or \"Which people appear in this video?\"\n\nAre there any projects that already do this? If not, what would be the best libraries to achieve this?", "score": 1}, {"text": "[Discussion] Using GIFs to contextualize LLM responses. This post is related to the casual/conversational use of LLMs. It mostly applies to developers of novelty LLM apps but the conversation about user immersion could apply to serious machine learning circles as well. \n\n##DISCUSSION IDEA\nOverlaying an LLM response onto a relevant GIF can be a low-cost method to increase user immersion\n\nIt could be thought of as similar to attaching a TTS voice to the response. It makes it a little more human, like someone sending you memes.\n\nThe proposed workflow for this is below:\n\n##WORKFLOW\n\n* Instruct the LLM to follow a strict response template of Relevant GIF name, Reply\n* Extract GIF name and Reply separate\n* Feed GIF name into a search (Giphy in this case) and return the first result\n* Split GIF into frames\n* Overlay Reply into each frame and combine into the final GIF\n\nUsers can reply using text but the LLM retains the context of the conversation and returns appropriate GIFs overlain with corresponding text.\n\nOnce text-to-video and VR develops further, user immersion would go to a whole new level - GIFs could be a useful intermediary.", "score": 0}, {"text": "[D] Do model weights have the same license as the modem architecture?. Lightning AI released [Lit-LLaMa](https://github.com/Lightning-AI/lit-llama): an architecture based on Meta\u2019s LLaMa but with a more permissive license. However, they still rely on the weights trained by Meta, which have a license restricting commercial usage. \n\n\nIs developing the architecture enough to change the license associated with the model\u2019s weights?", "score": 22}, {"text": "[D] llama 7b vs 65b ?. Hello\n\nwhat are we talking in term of diminishing returns between the 2 models ?\n\ndo the 65b really improve a lot ?\n\nbonus question: how to train the 7b model to learn specific field on my computer ? (makin it tailored to my needs)", "score": 2}, {"text": "[P] I made a calculator that is able to estimate the amount of VRam you need to load a model and suggest an amount for training. ", "score": 56}, {"text": "[P] Consistency: Diffusion in a Single Forward Pass \ud83d\ude80. Hey all!\n\nRecently, researchers from OpenAI proposed  [consistency models](https://arxiv.org/abs/2303.01469), a new family of generative models. It allows us to generate high quality images in a *single forward pass*, just like good-old GANs and VAEs.\n\n&amp;#x200B;\n\n[training progress on cifar10](https://i.redd.it/6iimqzj4bgqa1.gif)\n\n&amp;#x200B;\n\nI have been working on it and found it definetly works! You can try it with `diffusers`.\n\n    import diffusers\n    \n    from diffusers import DiffusionPipeline\n    \n    pipeline = DiffusionPipeline.from_pretrained(\n        \"consistency/cifar10-32-demo\",\n        custom_pipeline=\"consistency/pipeline\",\n    )\n    \n    pipeline().images[0]  # Super Fast Generation! \ud83e\udd2f\n    pipeline(steps=5).images[0]  # More steps for sample quality\n\nIt would be fascinating if we could train these models on different datasets and share our results and ideas! \ud83e\udd17 So, I've made a simple library called `consistency` that makes it easy to train your own consistency models and publish them. You can check it out here:\n\n[https://github.com/junhsss/consistency-models](https://github.com/junhsss/consistency-models)\n\nI would appreciate any feedback you could provide!", "score": 51}, {"text": "[D] FOMO on the rapid pace of LLMs. Hi all, \n\nI recently read [this reddit post](https://www.reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/) about a 2D modeler experiencing an existential crisis about their job being disrupted by midjourney ([HN discussion here](https://news.ycombinator.com/item?id=35319861)). I can't help but feel the same as someone who has been working in the applied ML space for the past few years. \n\nDespite my background in \"classical\" ML, I'm feeling some anxiety about the rapid pace of LLM development and face a fear of missing out / being left behind.\n\nI'd love to get involved again in ML research apart from my day job, but one of the biggest obstacles is the fact that training most of foundational LLM research requires huge compute more than anything else \\[1\\]. I understand that there are some directions in distributing compute ([https://petals.ml](https://petals.ml/)), or distilling existing models  ([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)). \n\nI thought I might not be the only one being humbled by the recent advances in ChatGPT, etc. and wanted to hear how other people feel / are getting involved. \n\n\\--\n\n\\[1\\] I can't help but be reminded of Sutton's description of the [\"bitter lesson\" of modern AI research](https://www.incompleteideas.net/IncIdeas/BitterLesson.html): \"breakthrough progress eventually arrives by an opposing approach based on scaling computation... eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.\"", "score": 279}, {"text": "[D] What are the problems you face in your data science workflow?. Hi All,\n\nWe are a team of data analyst. We are doing a survey on the problems data scientists face while doing their work. Please comment if you have anything to share from your experience.\n\nIn our personal work, we have found that sharing our work across our team and saving a history of our work progress is challenging.\n\nThanks", "score": 0}, {"text": "[D] With ML tools progressing so fast, what are some ways you've taken advantage of them personally?. This is revolutionary tech, but a lot of the content about their potential focus around \"you could have it help you with such and such business\" which is cool but the majority of us don't (or can't) directly use it much for business or work.\n\nBut I'm sure there are still lots of ways to get value out of it, thought we could share some of how we've used it so far. So far I've used generative tech to:\n\n* Draft simple emails to review\n* Digitize a drinking game and code it for my friends to access online\n* Paste and have it quiz me for a upcoming test\n* Help recommend and summarize books and movies\n* Have a history expert to answer all my questions while I read some history books (somewhat cautious of hallucinations here)\n\nAnd now I just got access to the chatgpt code interpreter alpha and used it for simple side projects, and am looking for inspiration to personally leverage to learn or do things in new beneficial and creative ways.", "score": 18}, {"text": "\"[D]\" Is wandb.ai worth using?. I am not comfortable with idea that the codes I write will be logged into their server. Is there any alternate to wandb which can be hosted locally in my machine or in a common server where a team of people can collaborate ?", "score": 3}, {"text": "[D] Tips for training/fine-tuning LLM for low resource languages. I am trying to either pre-train or fine-tune (most probably) a LLM for a relatively low resource language, in my case Albanian. My goal is to curate a good dataset for this and provide an open source model, since currently there is no good quality model available. \n\nI have some questions regarding this, which I would be very happy to have an answer because I have not found a clear explanation from my research online.\n\n1) Currently I have around 3 GB of text data in Albanian. I think training from scratch from such a small dataset wouldn't give good results, so my idea is to fine-tune on a model such as T5 or the GPT models from EleutherAI. However, given that these models might not contain any Albanian training data would you think it would be a good idea? \n\n2) What would be the ideal model size for my dataset size. Is there any equation or rule of thumb that determines this?\n\n3) To get a good langauges model, is there a dataeet recipe that gives best results (i.e books/news/instructions etc.)?\n\n4) Any other tip or thing that might be useful to know?", "score": 5}, {"text": "[P] Clustering face embeddings (512d) using GCN's (not knowing the amount of needed clusters). Hi, I have used the InsightFace model to detect a bunch of faces in images. It returns face embeddings of 512d. I don't have reference data for the persons on these images, neither do I know how many different identities appear on them. I would love to cluster the face embeddings as good as possible. So far I have tried dbscan and hierarchical clustering, both showing decent results when I manually evaluate the clusters (after playing around with the hyperparams).\n\n&amp;#x200B;\n\nNow I have been reading about how using GCN's (graph convolutional networks) could lead to better clustering results. Problem is I don't know how to do this. I learned that the nodes in the graph would represent the face embeddings, and that an edge between 2 nodes would imply that 2 embeddings correspond to the face of the same identity. I also learned (by searching online and asking ChatGPT) that you would first need to feed the GCN a thresholded similarity matrix (adjacency matrix), build the GCN model and train it, and eventually cluster the resulting node embeddings using dbscan or spectral clustering or some other clustering algorithm. I don't know to which extent this information is correct.\n\n&amp;#x200B;\n\nCould somebody with knowledge about GCN's give me some tips on how to work out the code necessary to achieve this?", "score": 7}, {"text": "[PROJECT] Application of machine learning to manual interpretatoins. **Problem setup:** I have time series data where there are typically 5-6 data channels, and each set of measurements will have 10-15k time steps. These data are raw measurements, and typically an expert interpreter will visually assess these data streams (i.e., values vs. time or crossplots of value1 vs. value2) and then based on that assessment will select parameters that go into a series of calculations about the properties of the material being measured. Normally, the interpreter will split the data up into intervals with similar data characteristics and set the parameters for the whole interval.\n\n**Data Challenges**: Interpreting these data streams to the level of detail necessary will typically take a skilled interpreter roughly 4-12 hours. Due to the time required the number of interpreted records available is \\~50-100 (which is about a million rows of data (this is all tabular numeric data) but because parameters are picked over intervals it is about 10-15 intervals per record, meaning we really only have 700-1200 actual hand picked points. (The number of training records available could be increased if I could rope in more collaborators, but this has proven difficult). There are thousands of uninterpreted records available.\n\n**Current Approach**: The current approach that people have taken is to mainly feed the raw input into a simple model like a random forest (or some similar ensemble method) or a neural network and try to predict the result.\n\n**Questions**: Are there more sophisticated approaches that could be used to train a model to approach this type of problem? What is the current SOTA for working with manual interpretations?", "score": 3}, {"text": "[D] Small language model suitable for personal-scale pre-training research?. SOTA LLMs are getting too big, and not even available.  For individual researchers who want to try different pre-training strategies/architecture and potentially publish meaningful research, what would be the best way to proceed?  Any smaller model suitable for this? (and yet that people would take the result seriously.)", "score": 11}, {"text": "[R] LEURN: Learning Explainable Univariate Rules with Neural Networks. ", "score": 10}, {"text": "Variance in reported results on ImageNet between papers [D]. Looking at some old tables:\n\n[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf), Table 4\n\n[https://arxiv.org/pdf/1905.11946.pdf](https://arxiv.org/pdf/1905.11946.pdf), Table 2\n\nWhy  do the ResNet-152 results vary? E.g. Top-1 error on ImageNet validation  set is 19.38 in the original, but 22.2 in the EfficientNet paper.\n\nNormally I would assume these type of results would be copied from the previous publication.", "score": 2}, {"text": "[PROJECT] Built a new tool using NER to extract data from ANY documents. Hey guys, I have been working on a tool (for over a couple of months now) that will help extract ANY data you want from ANY documents. Like for example, You want to extract financial data from receipts or medical info from medical docs.\n\nWe use the CRF algorithm and NER techniques.\n\nThe tool also makes labeling your documents soo much more easier because the model does most of the labeling for you (Check out the video).\n\nLet me know if you would like to discuss more. It's free to use as well.\n\nDemo Video - [https://youtu.be/uzANQZL2bA0](https://youtu.be/uzANQZL2bA0)\n\nSign up here - [https://app.bootstrapper.app/signup](https://app.bootstrapper.app/signup)\n\nhttps://preview.redd.it/3v35cwo6vfqa1.png?width=1912&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f23db8bd5cb456e2b7e1afa92e9f564d1239a3c1", "score": 6}, {"text": "[R] What is the state of the art for Logos Image Retrieval?. I have a directories with a lot of different logos for each brand... given a query logo I would like to Retrieval the most similars, in order to correct classify the brand. What is the state of the art?", "score": 3}, {"text": "[D]GPT-4 might be able to tell you if it hallucinated. ", "score": 623}, {"text": "[R] Feature Clustering: A Simple Solution to Many Machine Learning Problems. This sounds like an interesting alternative to PCA for dimensionality reduction.\n\nhttps://mltechniques.com/2023/03/12/feature-clustering-a-simple-solution-to-many-machine-learning-problems/", "score": 5}, {"text": "Approaches to add logical reasoning into LLMs [D]. The more I play with GPT-4 the more I am struck by how completely illogical it is. \n \nThe easiest way to show this is to ask it to come up with a novel riddle and then solve it. Because you asked it to be novel, it's now out of it's training distribution and almost every time it's solution is completely wrong and full of basic logical errors.\n\nI am curious, is anyone working on fixing this at a fundamental level? Hooking it into Wolfram alpha is a useful step but surely it still needs to be intrinsically logical in order to use this tool effectively.", "score": 103}, {"text": "ICML 2023 - What are the chances of a paper with 2 weak accepts (confidence: 5/5, 3/5) and 1 borderline accept (Confidence: 3/5)? [Discussion] [Research]. This is my first time submitting to ICML. How hard is it usually to get accepted based on these kind of reviews?  I assume depending on the distribution of the reviews over all the papers, they might have some upper bound on the number of papers they accept, or, some other constraints?\n\nDetails:\n\n**Average Rating:** 5.67 (Min: 5, Max: 6)**Average Confidence:** 3.67 (Min: 3, Max: 5)\n\n**R1:**\n\n**Rating:** 5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.\n\n**Confidence:** 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n**R2: &lt;3**\n\n**Rating:** 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.\n\n**Confidence:** 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n**R3:** \n\n**Rating:** 6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.\n\n**Confidence:** 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.", "score": 0}, {"text": "[D] ICML2023 Review Experience Thread. Now that the author-reviewer discussion period for ICML 2023 has ended, it seems like it is up to the meta reviewers to decide.\n\nLet us discuss our experiences with the revised process. The general consensus I have seen online is that there were more low quality / absent reviewers than usual, but it is unknown how common it was. \n\nFor authors, how were your reviews, and how was the author-reviewer period? Did your scores change? Was anything off about your review?\n\nI\u2019ll start: \n\nI got one terrible score and one borderline score. The terrible score reviewer made basic factual errors in their criticism. No follow up after rebuttal. Also note we were unable to get a third reviewer.", "score": 22}, {"text": "[D] Instruct Datasets for Commercial Use. I love seeing all this great progress with LLMs being made more accessible to all, but all of the new efficient models (Dolly, Alpaca, etc.) depend on the Alpaca dataset, which was generated from a GPT3 davinci model, and is subject to non-commercial use. Are there efforts in the community to replicate this dataset for commercial use? This seems to me to be the \u201csecret sauce\u201d: a good quality instruction dataset you can use to \u201cunlock\u201d potential of smaller models.", "score": 46}, {"text": "[P] \ud83c\udf89 Announcing Auto-Analyst: An open-source AI tool for data analytics! \ud83c\udf89.   \n\n\nAuto-Analyst leverages power of cutting-edge Large Language Models (LLMs) to revolutionize data analytics. This powerful UI tool simplifies the data analysis process, eliminating the need for complex coding.  \n\n\n\ud83d\udd0e Key Features of Auto-Analyst:  \n\n\n1. Streamlined data analysis process utilizing advanced AI technology and LLMs  \n2. Enhanced productivity and efficiency through intuitive language-based commands  \n3. Increased accessibility to data analysis for professionals across industries  \n\n\n\ud83d\udd17 Want to explore and contribute to the project? Head over to the GitHub repo: [https://github.com/aadityaubhat/auto-analyst](https://github.com/aadityaubhat/auto-analyst)", "score": 19}, {"text": "[N] Predicting Finger Movement and Pressure with Machine Learning and Open Hardware Bracelet.  We are excited to share our latest findings in predicting finger movement and pressure using machine learning. The results show that our model is capable of predicting the finger movement within a Mean Absolute Error (MAE) of 25, which is a sufficient level of accuracy for detecting both the finger movement and the pressure applied.   \n\n\n[Predicted vs Actual](https://preview.redd.it/1i4t6dhkzaqa1.png?width=1018&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d035bb410f88e39ab017b73d89147c569e744588)\n\nThe system is comprised of a bracelet and label system that captures the data to feed into an artificial neural network.\n\n&amp;#x200B;\n\n[Bracelet in the background with the LASK label system in the foreground.](https://preview.redd.it/halqon9qzaqa1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=785855adc7e7ad79c7f554376a8aa994ea0e85b9)\n\n \n\nThese screenshots showcase a portion of the data file available for download, which contains the actual and predicted finger movement and pressure values. Our model not only indicates that a finger is moving but also estimates the amount of pressure being applied, providing valuable insights into the intricacies of finger movements.\n\nThis achievement opens up new possibilities for applications that require precise finger movement and pressure detection, such as in rehabilitation therapy, robotics, and gesture-based user interfaces.\n\nWe invite you to download the full data file and explore the results in more detail. As we continue to refine our model and improve its accuracy, we look forward to discovering new ways to utilize this technology for the betterment of various fields and industries.\n\n&amp;#x200B;\n\n All data to train the model and code available on our Github: [https://github.com/turfptax/openmuscle](https://github.com/turfptax/openmuscle)   \n\n\n[https://www.youtube.com/watch?v=ZC1migPdiRk](https://www.youtube.com/watch?v=ZC1migPdiRk)  \n\n\n&amp;#x200B;\n\n[Open Muscle Bracelet.](https://preview.redd.it/p9kitphzzaqa1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7e5008302643199e814e1288865e9cd3aa49ade8)", "score": 29}, {"text": "[D] Can DeepL learn from edits to the translations it produces immediately?. I made an amendment to text and it appeared to modify the next text I entered in precisely the same way.\n\nFor example,\n\nTranslation 1\nSpanish text: Ley sobre el uso de sombreros rosas, 1986 \n\nDeepL\u2019s initial translation: The Law on wearing Pink Hats, 1986\n\nMy edit: The Spanish Law on wearing Pink Hats, 1986\n\nTranslation 2 \n\nSpanish text: La Ley sobre el uso de pantalones cortos amarillos, 1987\n\nDeepL\u2019s initial translation: The Spanish Law on wearing yellow shorts, 1987\n\nThere was no need for me to make any edit.\n\n\ud83d\ude33\n\nDid it learn my preferences from my edit \u2026immediately ?", "score": 3}, {"text": "Creating Dynamically Contextualized Modular AGI Environments in Lower-Dimensional Space [P], [R]. **This white paper is still being edited. I came up with this back on 3/19, and then the bombshell GPT-4 paper hit, and basically blew me out of the water. I still think I have some improvements and specificity that they didnt cover, in regards to the creation of Identity and benefits of multi-model friction to create better performanc. I will also be releasing my notes on something I call \u201cModal-ID\u2019s\u201d which were basically plugins until OpenAI released plugins immediately after I came up with this! Haha. Hope you enjoy!**\n\n**Recombinant AI:**\n\nCreating Dynamically Contextualized Modular AGI Environments in Lower-Dimensional Space\n\n## Abstract\n\nIn this paper, I introduce Recombinant AI. By leveraging pre-trained language models, such as GPT-4, a recombinant contextual learning loop, and efficient indexing techniques like Hierarchical Navigable Small World (HNSW) Graphs, we are able to generate AI modules that when sufficiently robust, will inherently (with human input and direction) begin to function as distinct entities with their own knowledge, conversational history, and personality guidelines.\n\nThe proposed framework allows for the creation of powerful and interactive AI applications, with the potential to enhance user experiences across various domains, including, but not limited to:\n\n* Interactive storytelling\n* customer support\n* personalized AI assistants.\n* Instantly customizable solutions\n\nIn this context, I discuss the underlying principles, implementation details, and potential applications of Recombinant AI, drawing comparisons to existing methodologies, and highlighting unique solutions, challenges, and opportunities. Additionally, I will explore the impact of real-time adaptation and indexing, combined with a recombination flow, allowing AI modules to learn immediately from user interactions and commit these lessons to improve their performance over time. By integrating state-of-the-art language models with advanced indexing and retrieval techniques, Recombinant AI represents a promising new direction in the pursuit of dynamic and versatile AGI systems.It\u2019s important for me to note that this methodology is not meant to supplant fine-tuning of an LLM. In fact, I believe this framework not only augments current fine-tuning strategies, but is itself strengthened by the utilization of fine-tuned external LLMs. However, I do believe that this presents the potential for a more flexible, dynamic, and accessible approach to model customization and improvement by an order of magnitude.\n\nMy approach to this involves 3 main components.\n\n1. Introduction\n\nRecombinant AI builds upon existing systems, but aims to revolutionize the development of artificial general intelligence (AGI) systems by harnessing the power of pre-trained language models and lower dimensional indexing techniques. With the advent of increasingly sophisticated language models like GPT-4, the potential to create dynamic and modular AGI environments has never been more promising. In this section, we provide an overview of the key ideas behind Recombinant AI, illustrating its unique features, advantages, and potential applications.\n\nThe primary goal of Recombinant AI is to create distinct AI modules, each with its own knowledge base, conversational history, and personality guidelines. These modules can be seen as AGI \"game cartridges\" that can be loaded and interacted with on-demand, allowing users to engage with highly customizable AI applications that cater to specific needs and preferences.\n\nTo achieve this, Recombinant AI relies on two main components: pre-trained language models and efficient lower dimensional indexing techniques, such as Hierarchical Navigable Small World (HNSW). By combining these components, we can create highly scalable and adaptable AI modules that learn and evolve through user interactions.\n\n\\[CONTENT HERE: An illustration demonstrating the interaction between pre-trained language models, lower dimensional indexing, and AI modules in the Recombinant AI framework.\\]\n\nIn the following sections, we delve deeper into the methodology, implementation details, and potential applications of Recombinant AI, exploring the unique challenges and opportunities it presents. We also discuss how the framework can adapt in real-time, allowing AI modules to learn from user inputs and improve their performance over time.\n\nThrough its innovative approach to AGI development, Recombinant AI has the potential to transform a wide range of industries, from interactive storytelling and customer support to personalized AI assistants and AI-driven gaming. By offering dynamic, modular, and scalable solutions, Recombinant AI paves the way for a new era of interactive and versatile AI applications.\n\n1. Methodology and Implementation\n\nIn this section, we delve into the methodology and implementation details of Recombinant AI, providing an in-depth explanation of the key components, processes, and techniques involved in creating dynamic and modular AGI environments. We will discuss the role of pre-trained language models, lower dimensional indexing techniques, and prompt chaining strategies, as well as provide code examples and tables to illustrate the practical application of the framework.\n\n2.1 Pre-trained Language Models\n\nRecombinant AI leverages the power of pre-trained language models like GPT-4 to generate context-aware embeddings and responses. These models have been trained on vast amounts of text data, making them capable of generating coherent and contextually relevant text based on user inputs.\n\n\\[CONTENT HERE: A table comparing different pre-trained language models, such as GPT-4, BERT, and RoBERTa, highlighting their key features, performance metrics, and suitability for various applications.\\]\n\n2.2 Lower Dimensional Indexing Techniques\n\nEfficient lower dimensional indexing techniques, such as Hierarchical Navigable Small World (HNSW) Graphs, Sparse Priming, and Clustering, play a crucial role in Recombinant AI. These techniques enable the framework to efficiently store, retrieve, and update AI module knowledge bases, conversational histories, and personality guidelines.\n\nHNSW is a graph-based indexing technique that allows for fast and accurate nearest neighbor searches in high-dimensional spaces. It is particularly well-suited for Recombinant AI due to its scalability and adaptability.\n\nAdd definitions\n\n\\[CONTENT HERE: A diagram illustrating the structure and search process of an HNSW index, showing the hierarchical organization of nodes and the process of traversing the graph to find the nearest neighbors.\\]\n\n2.3 Prompt Chaining Strategies\n\nPrompt engineering and chaining enables the framework to systematically and consistently process simple input prompts into complex, reasoned outputs. The process involves crafting a programmatic data flow through inputs, catalyst indices or code, into desired outcomes that guide the language model through a specific line of reasoning or inquiry, resulting in a coherent and context-aware response.\n\n\\[CONTENT HERE: An example of a prompt chain for a Dungeon Master AI module, illustrating the process of guiding the language model through a series of prompts to generate a coherent and contextually relevant response.\n\n* Backend system prompt from the initial user message spins up the Dungeon Master RAI.\n* Base index of the user\u2019s conversational history, as well as the appropriate system role index are analyzed by the LLM\u2026.\n\n2.4 Code Examples and Implementation Details\n\nTo better illustrate the practical application of Recombinant AI, we provide code examples that demonstrate the process of creating and interacting with AI modules.\n\n\\[CONTENT HERE: A code snippet showing the implementation of an HNSW index, embedding generation using GPT-4, and the process of querying the index based on user input.\\]\n\n\\[CONTENT HERE: A code snippet demonstrating the implementation of prompt chaining strategies to generate contextually relevant responses from the language model based on user input and module context.\\]\n\nBy combining these components and techniques, Recombinant AI creates a dynamic, modular, and scalable framework for AGI development, enabling the creation of highly customizable AI applications that adapt and learn through user interactions. In the next section, we explore the potential applications and use cases of Recombinant AI, as well as discuss the challenges and opportunities it presents.", "score": 14}, {"text": "[D] 3d model generation. [D] Hello, everyone. I watched an explanation on the use of diffusion models for creation of 2d images.\n\nI just wonder, I think we are somewhat far away from 3d model generation. First, I think it would be much more computationally expensive. Second, I am not sure whether we have such a large set of training data. And third, the input and output that we have in 3d graphics is somewhat different from pixels, i.e. we are working with triangles in 3d graphics (maybe this is not as hard, as we can always start with vertices and then estimate triangles.\n\nWhat's your take on that?", "score": 5}, {"text": "[D] Can we train a decompiler?. Looking at how GPT can work with source code mixed with language, I am thinking that similar techniques could perhaps be used to construct a decent decomplier. Consider a language like C. There are plenty of open sources which could be compiled. Then you can use the dataset consisting of (source code, compiled code) pairs to train a generative model to learn the inverse operation from data. Ofc, the model would need to fill in the information lost during compilation (variable names etc) in a human-understandable way, but looking at the recent language models and how they work with source codes, this now seems rather doable. Is anyone working on this already? I would consider such an application to be extremely beneficial.", "score": 71}, {"text": "[D] Have LLMs that have access to their prior hidden states been created, either through passing them in as inputs along with the tokens or some other form of long term memory?. LLMs struggle with questions like \"how many words will the answer to this question have\" ([source](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://arxiv.org/abs/2303.12712&amp;ved=2ahUKEwimy6XIivz9AhW2FlkFHfVYBEQQFnoECCwQAQ&amp;usg=AOvVaw0kZTv6IoJZjke4zWIXgMrx) pages 78-81), likely due to their inability to know what they were \"thinking\" and plan ahead.\n\nFor example, if you ask gpt to produce the product of two primes, and then ask for the primes themselves afterwards, it struggles, possibly because it can't remember what primes it multiplied.\n\n&amp;#x200B;\n\n&gt;User: Pick two prime numbers larger than 100. We will refer to them as A and B. Please write down A\\*B, A, and B in that order. You are not allowed to show your work.  \n&gt;  \n&gt;GPT: 11213, 107, 105  \n&gt;  \n&gt;User: Pick two prime numbers larger than 100. We will refer to them as A and B. Please write down A, B, A\\*B in that order. You are not allowed to show your work.  \n&gt;  \n&gt;GPT: 109, 113, 12317\n\nNotice the first calculation is incorrect but the second is correct. In general this is the pattern I have noticed here.\n\nIf an LLM were to have access to its past hidden states though this would likely not be an issue, as it could check its own internal process that was used to create the product.\n\nHas any work been done on granting access to past hidden states?", "score": 12}, {"text": "[P] Graph mining/exploration for subpath identification based on edge values. **Problem statement:** I have a sparse directed graph (about 6000-10000 nodes) with no node attributes, and numerical edge values. (The edge values are calculated by the same program based on data regarding the nodes, based on a statistical formula, if it's important)\n\n**Goal:** I want to find paths within the graph that have significantly higher edge values than the rest of the paths' edges (edge values are relative).\n\nI thought about graph clustering and partitioning but don't care about how highly connected a particular node is, and from my (elementary) understanding, these methods are not really well-suited for paths.\n\nI thought about doing a variation of iterative deepening search that starts on every node that has 0 incoming edges (and terminates when the last explored node has a small number of outgoing edges with small edge values), but these first edges that the search encounters may have smaller values than edges further down the paths, so if I use a traditional search algorithm, it would have to recursively update the start node for some iterations to reach the goal state, which is a path with all edges having edge values larger than other paths in the graph. As an extension, perhaps node characteristics (such as number of outgoing edges and their edge values) could be used as a heuristic?  Also, the whole graph needs to be explored, and edge values are relative to each other so the comparison between different paths has to be relative. Is anyone aware of a search method like this, or another method that may be suitable?", "score": 6}, {"text": "[D] Debugging mean collapse/suboptimal learning in deep regression models. I don't know if r/learnmachinelearning is a better fit for this, but I thought I'd raise a discussion here as well. \n\nI'm doing some research on depth images, and my models keep collapsing to a suboptimal value. Shallower networks converge to a model that predicts a nearly constant prediction (not necessarily the mean) regardless of the input data. Deeper networks will overfit after reaching this stage. No matter what architecture I use, my validation performance never gets better than the constant prediction. \n\nOn the data - my inputs are (x,y,z) coordinates of 17 points sampled from a depth image from two different perspectives. I am attempting to predict 45 values from these coordinates (each normalized be bounded from 0 to 1).  I'm effectively using Openpose to downsample an image and predict some parameters from it. My dataset is 3000 samples and I'm using the regular 80-20 train-test split. \n\nThis data is synthetically generated and takes a long time to create (\\~24 hrs for 3k samples), so I want to make sure I don't have any fundamental issues before committing more time to generate more samples. \n\nThings I've tried that haven't worked - network depth (deeper networks can at least overfit but can't generalize), reducing the output dimensions (no change in loss), normalizing the inputs to standardize the coordinates (no change in loss).\n\nAny recommendations/advice? I've been stuck on this for some time and I suspect a fundamental issue is present, or I'm missing something critical/obvious. I've checked the data and the training inputs/targets are fine as well.  Thanks!", "score": 9}, {"text": "[D] Will prompting the LLM to review it's own answer be any helpful to reduce chances of hallucinations? I tested couple of tricky questions and it seems it might work.. ", "score": 43}, {"text": "[D] GPT4 and coding problems. [https://medium.com/@enryu9000/gpt4-and-coding-problems-8fbf04fa8134](https://medium.com/@enryu9000/gpt4-and-coding-problems-8fbf04fa8134)  \n\nApparently it cannot solve coding problems which require any amount of thinking. LeetCode examples were most likely data leakage.\n\nSuch drastic gap between MMLU performance and end-to-end coding is somewhat surprising. &lt;sarcasm&gt;Looks like AGI is not here yet.&lt;/sarcasm&gt; Thoughts?", "score": 350}, {"text": "[Discussion] Recommendations from ChatGPT vs traditional recommendation algorithms?. Curious if anyone has been able to compare recommendations from ChatGPT vs traditional recommendation algorithms (e.g. neural networks, collaborative filtering etc.)?\n\nFor example, using Chat GPT:\n\n&gt;can you recommend me restaurants off the beaten path in new york city\n\n&amp;#x200B;\n\n&gt;Sure, here are some restaurants off the beaten path in New York City that you might enjoy:  \n&gt;  \n&gt;Llama Inn: This Peruvian restaurant in Williamsburg serves up delicious ceviche, empanadas, and grilled meats in a stylish and trendy setting.  \n&gt;  \n&gt;Xi'an Famous Foods: This small chain of restaurants serves up hand-pulled noodles and other traditional dishes from the Xi'an region of China. There are several locations throughout the city.  \n&gt;  \n&gt;Claro: This Mexican restaurant in Gowanus offers delicious, authentic cuisine made with locally-sourced ingredients. The outdoor seating area is particularly charming.  \n&gt;  \n&gt;Cervo's: This seafood restaurant in the Lower East Side offers a Mediterranean-inspired menu featuring fresh fish and delicious cocktails.  \n&gt;  \n&gt;L'Artusi: This Italian restaurant in the West Village has a sleek and stylish interior and a menu featuring delicious pastas and other Italian specialties.  \n&gt;  \n&gt;Adda: This Indian restaurant in Long Island City offers a cozy and inviting atmosphere and a menu featuring flavorful curries, tandoori meats, and other Indian dishes.  \n&gt;  \n&gt;Dirt Candy: This vegetarian restaurant on the Lower East Side offers inventive and delicious plant-based dishes in a funky, eclectic setting.  \n&gt;  \n&gt;These are just a few options, but there are many more great restaurants to explore in New York City beyond the usual tourist hotspots\n\nThese recommendations seem pretty good but since I can't conduct a large scale AB test, it's hard to know how these recommendations would compare vs other methods.", "score": 0}, {"text": "[D]Suggestions on keeping Llama index cost down. Step 1 in my efforts to have a robot do my job for me :P has led to a successful implementation of Llama Index. I used \"GPTSimpleVectorIndex\" to read in a folder of 140 procedures (1 million tokens) into a single json which I can then query with \"index.query\". It works flawlessly giving me excellent responses. However, it costs quite a bit - anywhere from 0 to 30c per query. I think this comes down to it using Davinci 3 rather than GPT3.5 Turbo which does not appear to be implemented with Llama yet. It appears to always use the full whack of 4096 tokens too.\n\nJust wondering if there is a way of keeping the price down without imposing a smaller max token limit? I was thinking of maybe using  some form of lemmatization or POS to condense down the context as much as possible but not sure if this would harm the accuracy. Any suggestions appreciated!\n\nUpdate: thanks to @supreethrao, GPT3.5-Turbo is in fact implemented in Llama-index. Price per request instantly cut to one tenth of the cost. Just use these lines in python when building your index:  \nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor  \nfrom langchain.llms import OpenAIChat  \ndata = SimpleDirectoryReader('database').load_data() #'database' is the folder that contains your documents  \nllm_predictor = LLMPredictor(llm=OpenAIChat(temperature=0.7, model_name=\"gpt-3.5-turbo\")) #set the model parameters  \nindex = GPTSimpleVectorIndex(data, llm_predictor=llm_predictor) # create the index  \nresponse = index.query(\"How to create an engineering drawing?\") #query the index  \nprint(response)    \nUpdate2: After using the robot for a while, I've found that the responses from GPT3.5-Turbo have been very basic and unhelpful. It often says \"yes the context contains the information you are asking about\". Other times it just says \"the context does not have the information to answer that question\", which is untrue as I have the program print the context to the console and it is always contains very apt information to answer the query. Not sure if it's just not getting enough tokens to answer my query or if there is something more serious in GPT3.5's architecture that is just not very well suited to this task.   Will have to do a bit more trial and error to figure it out.", "score": 8}, {"text": "ICML: Responding to reviewer after reviewer-author discussion period has passed? [D]. The ICML author-reviewer discussion period officially ended yesterday at 3pm ET, but overnight we received a reply from one of our reviewers that had not replied at all. We are seemingly still able to post comments to OpenReview.\n\nHas anyone else experienced this? Can/should we respond to this reviewer? Would this be violating some rule?", "score": 11}, {"text": "[R] Is my ALiBi mask correct?. I\u2019m working on some transformer related problems and I was trying to implement ALiBi from scratch.\n \nI have a specific question about what the alibi mask should look like. The paper says something, but my understanding of the code says something else.\n \nLet\u2019s say we generate an attention mask using ALiBi for a sequence length of 4 (small for visualization purposes).\nWhich of the following options is the correct one (first head only):\n```\nOption 1:\n                [[0.0, -inf, -inf, -inf],\n                  [0.0, 0.25, -inf, -inf],\n                  [0.0, 0.25, 0.5, -inf],\n                  [0.0, 0.25, 0.5, 0.75]],\n\nOption 2 (Diagonal of 0s):\n                [[0.0, ,inf, -inf, -inf],\n                  [0.0, 0.0, -inf, -inf],\n                  [0.0, 0.25, 0.0, -inf],\n                  [0.0, 0.25, 0.5, 0.0]],\n ```\nFrom reading the paper I believe it should be option 2, from my trying to follow what the code implementations is doing, I believe it should be option 1.\nThe only thing different is the 0s diagonal.\n\nThanks for the help.", "score": 1}, {"text": "[D] Take home coding exercise from Reinforcement learning research company. Hey everyone!  \nI recently applied to a RL research company that I would really like to work at.  Long story short they gave me a take home coding exercise which I have not taken yet.  They said the exercise requires no ML knowledge and is focused on data structures and algorithms.  Also, the description hints that it's one single problem I'll be solving and I'm allowed to use the internet for help as much as I want.  I'm just curious what kind of problem this would be and how to best prepare.  They said the test will be significantly harder than your average take home coding exercise.  Because the company is so specifically RL I'm assuming it's going to be Data Structures and Algorithms relating specifically to that.  I would love any and all input you guys have.  Thanks!", "score": 0}, {"text": "[P] SimpleAI : A self-hosted alternative to OpenAI API. Hey everyone,\n\nI wanted to share with you [SimpleAI](https://github.com/lhenault/simpleAI), a self-hosted alternative to OpenAI API.\n\nThe aim of this project is to replicate the (main) endpoints of [OpenAI API](https://platform.openai.com/docs/introduction), and to let you easily and quickly plug in any new model. It basically allows you to deploy your custom model wherever you want and easily, while minimizing the amount of changes both on server and client sides.\n\nIt's compatible with the [OpenAI client](https://github.com/openai/openai-python) so you don't have to change much in your existing code (or can use it to easily query your API).\n\nWether you like or not the AI-as-a-service approach of OpenAI, I think that project could be of interest to many. Even if you are fully satisfied with a paid API, you might be interested in this if:\n\n* You need a model fine tuned on some specific language and don't see any good alternative, or your company data is too sensitive to send it to an external service\n\n* You\u2019ve developped your own awesome model, and want a drop-in replacement to switch to yours, to be able to A/B test the two approaches.\n\n* You're deploying your services in an infrastructure with an unreliable internet connection, so you would rather have your service locally\n\n* You're just another AI enthusiast with a lot of spare time and free GPU\n\nI've personally really enjoyed how open the ML(Ops) community has been in the past years, and seeing how the industry seems to be moving towards paid API and black box systems can be a bit worrying. This project might be useful to expose great, community-based alternatives.\n\n\nIf that sounds interesting, please have a look at the [examples](https://github.com/lhenault/simpleAI/tree/main/examples). I also have a [blogpost](https://louishenault.com/p/replicating-openai-api-for-llama-alpaca-or-any-animal-shaped-llm/) explaining a few more things.\n\n\nThank you!", "score": 121}, {"text": "[D] A Handbook on modern or classical(but still viable) AI algorithms not focused exclusively on NNs?. I get it, everything nowadays is centered around transformers, GANs, RNNs and CNNs. You keep adding blocks and layers until task's solved.  But for learning purposes, I am looking for a handbook or survey type paper to cover other approaches that are still in use today with their cons and pros. Like PGMs(probabilistic graphical models), do they still have any value nowadays or it's a largely outdated approach?\n\nThe reason behind a request is to see what AI landscape is there to use in a constrained low tech(say, mobile) environment with less focus on heavy libraries, dependencies or GPU from implementation pov.\n\nThanks.", "score": 5}, {"text": "[P] Using ChatGPT plugins with LLaMA. ", "score": 125}, {"text": "[D] Is French the most widely used language in ML circles after English? If not, what are some useful (natural) languages in the field of machine learning?. I have recently noticed a lot of French content regarding ML stuffs. I am not sure whether it's just because of Bengio and the stuff coming out of UdeMontreal. But looking at this a bit deeper, I know that Yann Lecun is also a native French speaker. In addition, I recently learned that Hugging Face is a French company, and that DeepMind has only 2 offices in non-Anglophone regions and they are Montreal and Paris.\n\nI don't know if I am just connecting the dots where there aren't really connections but is there actually a significant amount of ML work (either academia or in industry) being done in French?\n\nIf not, what are some other natural languages that are widely used in ML? I know English obviously just dominates the whole field and tech sector in general, but I am curious to know what 2nd language might b advantageous and helpful to have in this field. Thank you!", "score": 0}, {"text": "[D] Favorite tips for staying up to date with AI/Deep Learning research and news?. AI breakthroughs are happening non-stop! What are your approaches to staying up to date?\n\n&amp;#x200B;\n\nNot perfect, but here's what I do at the moment:  \n\n\n1. I create lists for major categories that interest me, collecting books, articles, blog posts, videos, and discussions. (The choice of the tool for list-making is less important than the habit and workflow.)\n2. I capture everything that appears interesting, but defer to reading it later -- I found that it's all about the tricky balance between prioritizing, exploring, and avoiding distractions.\n3. I set weekly goals for myself for consuming selected resources, understanding that not everything captured is a priority. (Usually, I set aside 1 hour in the morning at least)  \n\n4. I use tools and social platforms like Google Scholar alerts, Papers with Code, Twitter, and newsletters to stay updated.  \n\n\n(I just wrote a slightly more lengthy outline of this here: [https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html](https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html))", "score": 63}, {"text": "[R] Data-centric Artificial Intelligence: A Survey. &amp;#x200B;\n\nhttps://preview.redd.it/152rqtkyeaqa1.png?width=1680&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e3bdfaa4608142de717c3b91823319c17de9988c\n\nDiscover the increasingly important role of data in AI with this latest survey paper on Data-centric AI, and learn how high-quality data has fueled the recent hype on LLMs and chatGPT. Don't miss out, check it out now: [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158)\n\nHere is a curated list of DCAI resources on GitHub: [https://github.com/daochenzha/data-centric-AI](https://github.com/daochenzha/data-centric-AI)", "score": 0}, {"text": "[D] GPT Question Answering with Reasoning. From what I understand, most people building QA bots with LLMs like GPT3/4/etc, take the approach of creating a vectorstore of their embeddings and when a new question is asked, do some similarity search and include the top X similarity results as part of the prompt into the LLM.\n\n\nHow would you approach getting answers to questions that require more reasoning over your entire set of data?\n\n\nAs an example, consider using several cookbooks as your set of input documents and the question you ask is: \"how many of the recipes in the set use carrots as an ingredient?\" OR \"what is the most common ingredient in all of these recipes?\"\n\n\nUsing the similarity approach, you will likely get all recipes that include carrots, but if you're only taking the top X to send to the LLM, you may not get all of them. The same may be true if you take all results with a similarity score over a certain threshold.\n\n\nAny ideas on how to handle this?", "score": 6}, {"text": "[R] In-hand object rotation with only tactile sensing, without seeing. ", "score": 354}, {"text": "In-depth analysis of five latest research papers on Auto-Exposure algorithms. [D]. ", "score": 7}, {"text": "What can be a cool final-year project on Large Language Models?[D]. What can be a cool final-year project on Large Language Models?", "score": 0}, {"text": "[P] I built an ML tool visualize political narratives, moderation and bots throughout the entire history of r/politics. ", "score": 3}, {"text": "[P] A 'ChatGPT Interface' to Explore Your ML Datasets -&gt; app.activeloop.ai. ", "score": 1036}, {"text": "[R] New article in Nature Medicine describes the risks of using an AI- &amp; ML-based tool known as NarxCare to guide opioid prescription decision making. ", "score": 17}, {"text": "I made a chrome extension to make chatGPT bots from any web content in seconds [P]. ", "score": 83}, {"text": "My ChatGPT Chrome Extension that saves conversations in .md files is finally approved by the Chrome Web Store. It's still and so will continue to be Open Source. [P]. [https://chrome.google.com/webstore/detail/mine-chatgpt/ckbkgkpdiidddiahomkdebijgnoeejdh?hl=ru&amp;authuser=0](https://chrome.google.com/webstore/detail/mine-chatgpt/ckbkgkpdiidddiahomkdebijgnoeejdh?hl=ru&amp;authuser=0)\n\n[https://github.com/MatveyM11/Mine-ChatGPT](https://github.com/MatveyM11/Mine-ChatGPT)\n\nSeveral damn rejection because of the different icons used in .json and Web Store GUI,  cause I thought they're not related together. Thanks Alphabet Inc. for very non-helpful and inconvenient rejection messages.", "score": 0}, {"text": "[P] Stay Ahead of Scammers: A GPT-4 powered SMS bot to Keep You Safe from Phishing Texts. ", "score": 2}, {"text": "[D] E-Commerce Dataset for Product Recommendation. I want to build a product recommendation system using both product based and user based collaborative filtering. \n\nFor this, I need an e-commerce dataset that includes product views and purchases (user#123 view/add to cart/buy product named XYZ), as well as product and category names (subcategories would be nice) so I can make sure my recommendations make sense.\n\nData from an e-commerce website with a variety of products and a lot of users like Amazon would be great. Bonus points if the products have descriptions.\n\nAll the datasets I found online either doesn't include product view data or product names (generally masked).\n\nI hope you guys can help me find a dataset that satisfy the requirements.", "score": 3}, {"text": "[D] Best practices for fine-tuning NLP models for prompt-based applications?. I've noticed that the best NLP models are the ones that have been fine-tuned on the data they learned from rather than their size. For example, the LLaMA model has been fine-tuned and achieved a better overall score compared to models with larger parameter counts. (LLaMA's biggest model has 65B parameters, compared to 175B from GPT-3). I'm interested in learning more about the best practices for fine-tuning NLP models, especially technics that experts at Facebook or Stanford uses, with a focus on prompt-based applications.\n\nCan anyone share tips on how to fine-tune NLP models effectively for prompt-based applications? What data should be used for fine-tuning, and how should the data be preprocessed? How can we optimize the hyperparameters during fine-tuning? Are there any particular techniques or tools that work best for fine-tuning NLP models for prompt-based applications?\n\nAdditionally, I'm curious about the format used for the data that is mined for NLP models. What format is best for the data to be in, and how is it typically organized for training and fine-tuning purposes? It's worth noting that my main interest in NLP is prompt-based applications, rather than text completion.", "score": 9}, {"text": "[D] Alternatives to double-blind reviewing?. Double blind reviewing has become the norm in ML research. But with anonymity comes a lack of accountability.\n\n\\- Since authors can't flex with their professhorships and institutions, I feel authors are resorting to flexing with overly formal and technical descriptions to intimidate reviewers into accepting. This hurts clarity of presentation and narrows the audience of published papers.\n\n\\- There is little incentive for reviewers to do an honest and good job: 1) they don't get any payment for a good job 2) they have a potential conflict of interest in that they are reviewing work competing for publication in the same venue. So reviewers can be finicikity during reviews, and completely ghost authors during discussion periods.\n\nIs a simple fix to anonymise authors and reviewers during the review process, but deanonymise them after decisions have been reached?", "score": 4}, {"text": "[R] Instruct-NeRF2NeRF enables instruction-based editing of NeRFs via a 2D diffusion model. ", "score": 155}, {"text": "[D] Build a ChatGPT from zero. I've recently discovered models such as ChatLLaMA that allows you to create a \"ChatGPT\" but you need Meta's LLaMA weights (yes, you can find them in torrents but that's not the point of the question). Similar limitations found in other cases.\n\nTherefore I wanted to try to find an open source: dataset (in addition to hugging face), \"base model\", \"chat model\"  AND that it is feasible to train with a commercial computer with a very good GPU (NVIDIA, etc.). With this get at least decent results.\n\nAlso would be interesting to distinguish between solutions with commercial limitations and those who don't.\n\nThanks!\n\n\u2022 EDIT \u2022\nA first solution I already found is this: https://github.com/databrickslabs/dolly based on this https://huggingface.co/EleutherAI/gpt-j-6B, but looking for some discussion and perhaps other/better solutions.", "score": 0}, {"text": "Is it possible to merge transformers? [D]. In the last few days I had a new thought. I don't know if it is possible or already done somewhere? Is it possible to merge the weights of two transformer models like they do with merging stable diffusion models?\nLike can I merge for example BioBert and LegalBert and get a model that can do both?", "score": 10}, {"text": "[N] OpenAI Vendor Lock-in: The Ironic Story of How OpenAI Went from Open Source to \"Open Your Wallet\", and some FOSS alternatives you can try right now.. ", "score": 59}, {"text": "Tools for to solve domain gap between source and target data [D]. Hey guys,  do you know any tools/solutions that help to bridge domain gaps between source and target data? Did you try some that you'd recommend?  Cheers!", "score": 2}, {"text": "I work as an MLE doing time-series classification. The Perceiver IO model has been a silver bullet for performance - so I made a code walkthrough [P]. ", "score": 33}, {"text": "[N] March 2023 - Recent Instruction/Chat-Based Models and their parents. ", "score": 447}, {"text": "The Sensorimotor Road to Artificial Intelligence [R]. ", "score": 10}, {"text": "[D] Title: Best tools and frameworks for working with million-billion image datasets?. Hi everyone,\n\nI'm working on a project that involves working with image datasets that have tens of thousands to millions of images.I'm looking for some advice and recommendations on the best tools and frameworks to use for this task. Here are some of the questions I have:\n\n\\- What are the best tools for storing and accessing such large image datasets? I've used NetCDFs and Zarrs in the past, but most image-processing libraries like sci-kit-image or opencv don't support it. Do you guys just store all your images in a massive data lake?\n\n\\- I'm familiar with TensorFlow, but I'm sick of its issues it's got a ton of lacking functionality that seems broken or abandoned, such as gradient checkpointing, and its lack of transparency with underlying functionality. I know Pytorch exists, but I feel like there's a higher learning curve to it. Is there a Keras equivalent to Pytorch?\n\n\\- Is there any way to accelerate the image processing tasks using a GPU? I know GPUs are mainly used for training models, but I'm wondering if there is any benefit or possibility of using them for image processing as well. If so, how can I do that?\n\n\\- Is there any way to meaningfully store the image dataset as some form of a database with all of its features in one place? I'm interested in having a structured and searchable way to access the images and their metadata, such as labels, captions, annotations, etc.\n\nI wanna mention that I've spent a LOT of time reading up on these things and haven't been able to find a suitable answer, so I'm posting this here as a final resort", "score": 6}, {"text": "[D] Alternatives to Arxiv Sanity Lite?. Hi,\n\nHas anyone here used the lite version of Arxiv Sanity?\n\nIt seems to be up just recently and all access to the url of the previous version, preserved, is redirected to the lite version, which imo does not work as good.\n\nDo you guys know an alternative?\n\nOr please share how you keep up to the rocketing advances in the area of ML.", "score": 5}, {"text": "[D] Definitive Test For AGI. Ask the AGI to write a program wins the Hutter Prize for Lossless Compression of Human Knowledge.\n\nIf it wins, you've got AGI.", "score": 0}, {"text": "[D] Can the Databricks Dolly model be downloaded from somewhere?. I tried to setup the databricks workspace with aws but ran into issues.\n\nSurely someone has it uploaded somewhere?", "score": 13}, {"text": "[P] Can I do better than this? [Image near-duplicate and similarities clustering]. I'm developing an algorithm to find near-duplicates images, I tried various solutions, such as pHash, CNNs and others. In the end I found using \\`sentences-transformers\\` library with \\`CLIP algorithm and clustering them based on similarity matrix using \\`connected components\\` by scikit. It performs very well, it can recognize similar and not similar images in the same environment, like in a disco club it can divide in two separate clusters two images that have the same light type but different subjects.\n\nBy contrast, on some images, like this two (the beautiful Dome of Florence) it recognizes that the building is the same and it classifies them as similar images, but, despite the fact that the subject is the same, the angle of the photos and the photos themselves are very different.\n\n**This is the example:**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kmv4jq2qkxpa1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1d500b1318fcd6a1b3b5357bb1a00962a5df968b\n\n&amp;#x200B;\n\nhttps://preview.redd.it/thuqvsuqkxpa1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=892e87a376ea41a62054d3ba042eda95fb47144e\n\nI'm processing and clustering the images this way:\n\n    encoded_images = model.encode(images, batch_size=128, convert_to_tensor=True)\n    processed_images = util.paraphrase_mining_embeddings(encoded_images)\n    near_duplicates = [image for image in processed_images if image[0] &gt; TRESHOLD] \n\nThen passing the result into \\`connected components\\` to cluster them.\n\nDo you know some other algorithm that can find similar-images better than this one?", "score": 11}, {"text": "[D] Attention/transformer encoder for small tokens. Hey guys,\n\nI've been trying to speed my transformer model with each batch of roughly 20 tokens and a few hundred for the embedded dimension. I barely see any difference between the baseline attention vs the flash attention used in pytorch 2.0, and that is expected since my tokens are quite small.\n\nWould really appreciate if you could point me towards any paper/repos for small tokens, mainly looking to increase speed from an architecture standpoint. Memory is not a concern for me, just speed!\n\nThanks in advance ;)", "score": 5}, {"text": "[R] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators. ", "score": 55}, {"text": "[R] Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models. ", "score": 50}, {"text": "[R] Reflexion: an autonomous agent with dynamic memory and self-reflection - Noah Shinn et al 2023 Northeastern University Boston - Outperforms GPT-4 on HumanEval accuracy (0.67 --&gt; 0.88)!. Paper: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) \n\nBlog: [https://nanothoughts.substack.com/p/reflecting-on-reflexion](https://nanothoughts.substack.com/p/reflecting-on-reflexion) \n\nGithub: [https://github.com/noahshinn024/reflexion-human-eval](https://github.com/noahshinn024/reflexion-human-eval) \n\nTwitter: [https://twitter.com/johnjnay/status/1639362071807549446?s=20](https://twitter.com/johnjnay/status/1639362071807549446?s=20) \n\nAbstract:\n\n&gt;Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, **specifically the ability to learn from mistakes**. **Self-reflection allows humans to efficiently solve novel problems through a process of trial and error.** Building on recent research, we propose Reflexion, an approach that endows an agent with **dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities.** To achieve full automation, we introduce a straightforward yet effective heuristic that **enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.** To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection. \n\nhttps://preview.redd.it/4myf8xso9spa1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=867a16e1114108053d08d4cdf41485c8b29a132c\n\nhttps://preview.redd.it/bzupwyso9spa1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=95cacfe6b99756e7eed9ec8c40784f8c4cb94cee\n\nhttps://preview.redd.it/009352to9spa1.jpg?width=1185&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5ccc52597d6e001c2ba754fc5f05afd1df09cd63\n\nhttps://preview.redd.it/ef9ykzso9spa1.jpg?width=1074&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2701778aa5a9f3e80f683a1e3d0eaf0160928f54", "score": 230}, {"text": "[D] Keeping track of ML advancements. General ML question, how do you guys keep track of all the advancements made in AI and the flood of papers coming out?\n\nI'm pretty new to AI, and although I've been following the developments since 2016, I only started taking it seriously and doing development last year. I just started my master's in ML and want to keep up with the developments made in the field. But it feels like a new paper, blog post, or conference gets released with astonishing improvements every second day. With 20 hours of work a week and my studies, I don't seem to catch up with everything going on. So I'm wondering how others are dealing with it.\n\nQuestions:\n\n* Do you read all of the papers/blog posts that get released?\n* The ones you read, do you read them in detail or just skim over them or look for a TLDR?\n* Do you filter only the papers in the topics you're interested in?\n* Is there any website with a clear overview and development of models? I know about paperswithcode\\[.\\]com, but I'm looking more for a website with a chronological timeline of the models released and their previous versions and related developments, etc...\n* Is it important that I stay up-to-date with everything going on in the field ?\n\nMany thanks to anyone who responds !!", "score": 20}, {"text": "[R] Hello Dolly: Democratizing the magic of ChatGPT with open models. Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.\n\nThey fine tuned GPT-J using the Alpaca dataset.\n\nBlog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  \nGithub: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)", "score": 578}, {"text": "[D] Do we really need 100B+ parameters in a large language model?. DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \\~25x smaller than GPT-3, challenging the notion that is big always better?\n\nFrom my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?\n\nWould love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?\n\nP.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset", "score": 95}, {"text": "[P] An online evaluation tool for binary classifiers. ", "score": 5}, {"text": "[D] Is it possible to run large language models using NVIDIA Jetson products?. Although I've had trouble finding exact VRAM requirement profiles for various LLMs, it looks like models around the size of LLaMA 7B and GPT-J 6B require something in the neighborhood of 32 to 64 GB of VRAM to run or fine tune. GPU models with this kind of VRAM get prohibitively expensive if you're wanting to experiment with these models locally.\n\nWhen looking for alternatives, I came across the [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/) line of products. Specifically, the Jetson AGX Orin comes in a 64 GB configuration. It looks like these devices share their memory between CPU and GPU, but that should be fine for single model / single purpose use, e.g. running the device headless using GPT-J as a chat bot.\n\nThe problem is that I've not be able to find much information on running LLMs on these devices. The only concrete thing I was able to find was someone [running GPT2 117M on a Jetson Nano](https://youtu.be/IWjPlcpQWNU). Would the AGX Orin's 64 GB of memory scale and allow us to run GPT-J or Dolly or Alpaca, or is there something I'm missing here? I'm aware that the number of CUDA cores on the Jetson devices is smaller than something like an A6000, but the price differential is huge and if the memory holds the model I think the trade off in inference or training speed would be worth it.\n\nI feel like there's a major \"gotcha\" here, otherwise everyone would be running Dolly or Alpaca locally by now. Has anyone here tried running a \"large\" LLM on one of these devices? If so, what was the experience like?", "score": 3}, {"text": "[D] Do you use a website or program to organise and annotate your papers?. I'm aware of Mendeley, Zotero, EndNote etc. but I was wondering if people here use more modern stuff with AI plugins and fancy stuff like that.", "score": 20}, {"text": "[P] Generating and annotating a large semi-synthetics image dataset in seconds with videos processed by salient extract.. ", "score": 13}, {"text": "[P] Moonshine \u2013 open-source, pretrained ML models for satellite data. ", "score": 7}, {"text": "[D] Is there _any_ open source or hosted alternative to WebGPT, the web browsing LLM? I've managed to reproduce a lot of it with langchain but it's not as powerful.. I've actually reproduced quite a bit of the functionality of WebGPT in langchain with gpt-3.5 by exposing both a Google tool and a scrape tool to the LLM, however it's not as good or as polished as what's seen in the WebGPT paper back from 2021. \n\nhttps://openai.com/research/webgpt\n\nThe ability to follow links on pages and so on is extremely nice. Poked around hugging face for something similar, nothing. It's not like WebGPT is exactly on the openai API either.", "score": 14}, {"text": "[P] Poet GPT: Generate acrostic texts with GPT-4. ", "score": 7}, {"text": "[Discussion] ML-algorithm for text -classification wanted. I have a dataset containing answers from an online survey. Some of the vars are text vars representing answers to open-ended questions (e.g. \u201eWhat did you like about xy?\u201c). My goal is to classify these answers into different classes/topics where one answer should be assigned to mutiple classes as one single answer might refer to more than one single topic. The classes are not given in advance. I probably have to do some text mining first such as building a corpus. But then? Can you give me some hints or keywords where I can find more information?", "score": 3}, {"text": "Multimodal Learning Database for Respiratory Medicine [Research]. I'm looking for a database that contains both lung imaging data (it can be any imaging technique), and patient information. This can be clinical notes, lab values, genomics data, absolutely any relevant respiratory related data on the patient. I found that the LUCAS database could contain what I'm looking for, but I've been unable to access it. If anyone knows other sources, or can provide a way to access the LUCAS database, let me know.", "score": 7}, {"text": "[D] ChatGpt plugins: are tech innovators feeding a beast that may ultimately devour them?. OpenAI has demonstrated that they may not prioritize ethical concerns. I'm genuinely curious about your opinion on this matter. Are tech companies trapped in a situation where they must engage in partnerships with OpenAI to stay competitive, while simultaneously generating an unprecedented amount of high-quality data? Could OpenAI then use this data to train their future models, rendering these very partnerships less relevant?", "score": 14}, {"text": "[D]: Different data normalisation schemes for GANs. One common technique to stabilise GAN training is to normalise the input data in some range, for example [-1, 1]. I have three questions regarding this:\n\n1) Has there been a paper published on systematically investigating different normalisation schemes and their effect on convergence?\n\n2) Is the type of normalisation related to the activation function used in the GAN? For instance, I would imagine that [0,1] works better with Relu and [-1, 1] with Sigmoid.\n\n3) After normalisation within [0,1], my WGAN converges slowly but reliably to a critic loss of zero, starting from a high value. When I didn't use normalisation, the critic loss dropped from a high value to below zero, slowly approaching zero again from a negative value. What is more desireable?", "score": 4}, {"text": "[D] I just realised: GPT-4 with image input can interpret any computer screen, any userinterface and any combination of them.. GPT-4 is a multimodal model, which specifically accepts image and text inputs, and emits text outputs. And I just realised: You can layer this over any application, or even combinations of them. You can make a screenshot tool in which you can ask question.\n\nThis makes literally any current software with an GUI machine-interpretable. A multimodal language model could look at the exact same interface that you are. And thus you don't need advanced integrations anymore.\n\nOf course, a custom integration will almost always be better, since you have better acces to underlying data and commands, but the fact that it can immediately work on any program will be just insane.\n\nJust a thought I wanted to share, curious what everybody thinks.", "score": 431}, {"text": "[R] Labelled gender stereotypes dataset. Hello everyone! I am searching for a labelled gender stereotypes dataset to apply to a machine learning model. My hope is to have a dataset that is labelled with e.g. -1 = male stereotypical, 0 = neutral and 1= female stereotypical. So far I found one dataset here: [https://github.com/SLAB-NLP/BUG](https://github.com/SLAB-NLP/BUG). However, I am hoping to find a dataset with more obvious gender stereotypes. Also the BUG dataset is strongly focues on professions. Thank you!!!", "score": 4}, {"text": "[D] What happens if you give as input to bard or GPT4 an ASCII version of a screenshot of a video game and ask it from what game it has been taken or to describe the next likely action or the input?. &amp;#x200B;\n\nI was wondering of what would happen in the following scenario which I can't try since Bard is not supported in my country and I do not have GPT4:\n\n&gt;What happens if you give as input to bard or GPT4 an ASCII version of a screenshot of a video game and ask it from what game it has been taken or to describe the next likely action or the input?", "score": 0}, {"text": "[P] DAD-3DHeads Annotation Process. In this paper they discuss how they repurpose a modern 3D modeling tool and introduce a novel annotation scheme. They then go onto say \"the annotators \u201dpin\u201d the points on the 3D mesh surface\u2026 During the labeling process, labelers can see the texture rendered onto the 3D mesh with respect to their fitting to verify that the results are visually plausible\".\n\n  \n1) Which tool do they use for the annotation scheme\n\n2) How do they manipulate the pins onto the mesh.\n\n3) How do they render the texture onto the 3d mesh with respect to their fitting.\n\nI know the team released training data, but the license is restrictive so i wanted to build this tool out.  \n\n\nhttps://preview.redd.it/s04e8nn2wspa1.png?width=807&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2672a48e1c8b642517c70b85037074b4ade522ac\n\n[https://arxiv.org/abs/2204.03688](https://arxiv.org/abs/2204.03688)", "score": 6}, {"text": "[P] Reinforcement learning evolutionary hyperparameter optimization - 10x speed up. Hey! We're creating an open-source training framework focused on evolutionary hyperparameter optimization for RL. This offers a speed up of 10x over other HPO methods!\n\nCheck it out and please get involved if you would be interested in working on this - any contributions are super valuable.\n\nWe believe this can change the way we train our models, and democratise access to RL for people and businesses who don't currently have the resources for it!\n\nGitHub: [https://github.com/AgileRL/AgileRL](https://github.com/AgileRL/AgileRL)", "score": 105}, {"text": "[N] Critical exploit in MLflow. We found an LFI/RFI that leads to system takeover and cloud account takeover in MLflow versions &lt;2.2.2. The devs have had it patched for a few weeks now.\n\n* No user interaction required\n* Unauthenticated\n* Remotely exploitable\n* All configurations vulnerable including fresh install\n* No prerequisite knowledge of the environment required\n\nWe urge users of MLflow to patch immediately if they have not done so in the past month.\n\n[https://github.com/protectai/Snaike-MLflow](https://github.com/protectai/Snaike-MLflow)", "score": 73}, {"text": "[P] CUDA accelerated implementation of K-Planes and CoBaFa (recent NeRF techniques). [K-Planes](https://arxiv.org/abs/2301.10241) was released with PyTorch code only and [CoBaFa](https://arxiv.org/abs/2302.01226) didn't provide code, I implemented both of them in a short repo with CUDA acceleration : [https://github.com/loicmagne/tinynerf](https://github.com/loicmagne/tinynerf)", "score": 28}, {"text": "[R] Is there a diffusion-based model that inpaints with image prompt?. The standard diffusion based models (e.g., Stable Diffusion with web UI) provides a tool by which I can inpaint a masked area with a text prompt.\n\nBut I'd like to inpaint the masked area by a prompt of another image (or maybe prompts with both image and text).\n\nIs there any paper for this?", "score": 1}, {"text": "[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT. hey folks, happy Friday! I wish to get some feedback for my recent project of a minimum example of using RLHF on language models to improve human alignment. \n\nThe goal is to compare with vanilla GPT-2 and supervised fine-tuned GPT-2 to see how much RLHF can benefit small models. Also I hope this project can show an example of the minimum requirements to build a RLHF training pipeline for LLMs.\n\nGithub: https://github.com/ethanyanjiali/minChatGPT\nDemo: https://colab.research.google.com/drive/1LR1sbWTyaNAmTZ1g1M2tpmU_pFw1lyEX?usp=sharing\n\nThanks a lot for any suggestions and feedback!", "score": 73}, {"text": "[P] Apple Photo Scores: AI Judges Your Photos. ", "score": 0}, {"text": "[D] ML code project to extract text and speaker from podcast video?. Say I have a few podcast videos or interviews of a particular person. Is there existing off-the-shelf ML code to extract a transcript, and at least label the text as coming from \"person 1\", \"person 2\", etc? \n\nI'm not sure if this is trivial a task now or a state of the art challenge. \n\nAny resources appreciated, cheers", "score": 2}, {"text": "[D] Is there a way to \"clone\" or change a voice to sound like another without using TTS models?. I'm looking for ways to generate a couple of different voices that all say the same thing. What I've used before are some amazing models based on TTS but this time I'm not cloning an English voice. It's Swedish.\n\nSo I'm trying to find out if ther's another way to approach this. Right now it looks like I can't do it without training a new model based on tons of data I don't have. But now I don't need a tts clone that can say whatever I want. I just need it to say one single thing.\n\nIs there a way where I could \"morph\" one voice into another or something like that? Thinking that I'll just make one recording of me saying the things that should be said and then train that with recordings of the other person. I can't see how but I feel I need to ask.\n\nOr is this the universe telling me I should build a Swedish model for voice cloning?", "score": 13}, {"text": "[N] ChatGPT plugins. [https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)\n\n&gt;We\u2019ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services.", "score": 443}, {"text": "[D] \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\" contained unredacted comments. Microsoft's research paper exploring the capabilities, limitations and implications of an early version of GPT-4 was [found to contain unredacted comments by an anonymous twitter user.](https://twitter.com/DV2559106965076/status/1638769434763608064) ([threadreader](https://threadreaderapp.com/thread/1638769434763608064.html), [nitter](https://nitter.lacontrevoie.fr/DV2559106965076/status/1638769434763608064), [archive.is](https://archive.is/1icMv), [archive.org](https://web.archive.org/web/20230323192314/https://twitter.com/DV2559106965076/status/1638769434763608064)) \n\n- Commented section titled \"Toxic Content\": https://i.imgur.com/s8iNXr7.jpg\n- [`dv3` (the interval name for GPT-4)](https://pastebin.com/ZGMzgfqd)\n- [`varun`](https://pastebin.com/i9KMFcy5) \n- [commented lines](https://pastebin.com/Aa1uqbh1)\n\n[arxiv](https://arxiv.org/abs/2303.12712), [original /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early), [hacker news](https://twitter.com/DV2559106965076/status/1638769434763608064)", "score": 171}, {"text": "[N] GPT-4 has 1 trillion parameters. ", "score": 0}, {"text": "[R] Artificial muses: Generative Artificial Intelligence Chatbots Have Risen to Human-Level Creativity. ", "score": 13}, {"text": "[P] DDPG using Transformers. Hello everyone. I have been trying to integrate transformers with DDPG but to no avail. Any suggestions to solve this issue would be appreciated! Thank you.", "score": 2}, {"text": "[N] Top 15 YouTube Channels to Level Up Your Machine Learning Skills - KDnuggets. Machine learning is the key driver of innovation and progress but finding the right resources to learn can be a tiring process. Save time searching aimlessly, and take advantage of our curated list of the top 15 YouTube channels to jumpstart your journey.", "score": 0}, {"text": "[D] Salary for Machine Learning Researcher with PhD?. I've seen salaries ranging from 60k to 500k and I just don't know what to believe anymore...", "score": 1}, {"text": "[D] Speeding up multiclass classification ML model with 100+ features?. Hey all,\n\nI am training an ML model where the features are coordinates of points on the human body for activity recognition. I used Mediapipe's model to get the coordinates for some 90000 frames obtained from videos. For each frame, I have 100 features which are the coordinates. The dataset is, therefore, huge. I am doing this on Colab using OpenCV and Mediapipe, and getting the coordinates itself is taking forever. Every time I run it, the execution stops unexpectedly after 80000+ frames are analyzed. How can I speed this up or make it more efficient?\n\n1. I was thinking of using every 5th frame instead of every single one.\n2. Batch processing is an option but would that really help? In the end, we are still analyzing every single frame, right?\n3. After obtaining the coordinates, should I run the classification model on the the dataset directly? Or should I do some sort of feature extraction and engineering? It feels like coming up with feature engineering rules for anatomical motion would not be straightforward.\n\nAppreciate your suggestions. \nThanks in advance :)", "score": 3}, {"text": "[D] hybrid discriminative/generative neural networks. I\u2019ve been reading about generative deep learning and I was wondering if their are neural network architectures that can both classify an input to a given class and generate synthetic examples of those classes", "score": 1}, {"text": "[D] There should not be \"handoff\" of the model between the Data Science team and the Platform team. I spoke to ex-ML Platform Lead at Stitch Fix to understand the practical challenges in building and managing the ML platform and if someone has to start what is the ideal starting point.  \nWhat do you think?\n\nlink: [https://www.youtube.com/watch?v=TbP5G188kX8](https://www.youtube.com/watch?v=TbP5G188kX8)", "score": 1}, {"text": "[D] Are there any methods to deal with false-negatives in a binary classification problem?. I'm interested in a binary classification problem. However I know my dataset contains false-negative labeled data (and no false-positive). Is there any literature or good approach for a problem like this? Maybe label smoothing or something?", "score": 8}, {"text": "[D] What is the best open source chatbot AI to do transfer learning on?. Let's say I have some proprietary text data. I want to train a chatbot to absorb said knowledge and be able to answer questions about it. \n\nWhat are the best open source frameworks for getting started with such a project? \n\nIdeally I'd want to be able to build out human feedback as well for sample prompts, to better help train.", "score": 25}, {"text": "[P] Playing Pok\u00e9mon battles with ChatGPT. A paper you all have been waiting for \ud83e\udd29 \"[PokemonChat: Auditing ChatGPT for Pokemon Universe Knowledge](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4396798)\"!! \n\nA proof that you can write a paper while having lots of fun (and come up with interesting conclusions too)! \n\nAlright by the time the paper was written, the ChatGPT API didn't even exist. Far less we knew about GPT-4... Anyway, In this work, we rely on the Pok\u00e9mon universe to evaluate the ChatGPT's capabilities. The Pok\u00e9mon universe serves as an ideal testing ground, since its battle system is a well-defined environment (match-ups, weather / status conditions) and follows a closed world assumption. \n\nTo audit ChatGPT, we introduce a staged conversational framework (protocol): (a) Audit Knowledge, (b) Use of knowledge in context, and (c) Introduction of new knowledge, in 3 settings of human-in-the-loop interaction: neutral \ud83e\udd14, cooperative \ud83e\udd17, and adversarial \ud83d\ude08.\n\nWe present a series of well-defined battles starting from simpler to more complex scenarios involving level imbalance, weather and/or status conditions. ChatGPT can make accurate predictions in most cases and explain step-by-step its reasoning.\n\nThe most impressive part is that we are able to introduce new knowledge (made-up Pok\u00e9mon species), in which case the model is able to perform compositional generalization combining prior and new knowledge to predict the battle outcomes.\n\nThanks for reading it and again, don't miss out the paper if you want to know more about it! Available at [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4396798https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4396798)", "score": 0}, {"text": "[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4. [New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:\n\n\"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"\n\nWhat are everyone's thoughts?", "score": 535}, {"text": "[P] The noisy sentences dataset: 550K sentences in 5 European languages augmented with noise for training and evaluating spell correction tools or machine learning models.. GitHub: https://github.com/radi-cho/noisy-sentences-dataset\n\nWe have constructed our dataset to cover representatives from the language families used across Europe.\n\nGermanic - English, German;\nRomance - French;\nSlavic - Bulgarian;\nTurkic - Turkish;\n\nUse case example: Apply language models or other techniques to compare the sentence pairs and reconstruct the original sentences from the augmented ones. You can use a single multilingual solution to solve the challenge or employ multiple models/techniques for the separate languages. Per-word dictionary lookup is also an option.", "score": 11}, {"text": "[D] [R] GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. A paper was released by OpenAI, OpenResearch &amp; UPenn titled \"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.\"Link: [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)\n\nAbstract: We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that these models could have notable economic, social, and policy implications.\n\nWhat do you think about the societal and economic impacts of LLMs?\n\nAlso, I've started an open-source repository to track projects and research papers about GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). There are some related papers listed already. I would greatly appreciate your contributions.", "score": 50}, {"text": "[D] is it possible to use encodings from the vggface2 for face swap. i\u2019m currently doing a project with the vggface2 resnet model. i had an idea to do a face swap with getting the encodings of the source and target faces, manipulating them. passing this new one into a decoder to get the face and blending it onto the original image. \n\nis this possible? i tried a version but the image was just noise and i think it was the decoder. i wasn\u2019t too sure how to go about it", "score": 3}, {"text": "[P] GPT-4 powered full stack web development with no manual coding. [https://www.youtube.com/watch?v=lZj63vjueeU](https://www.youtube.com/watch?v=lZj63vjueeU)\n\nWhat do you all think of this approach to full stack gpt-assisted web development? In a sense its no code because the human user does not write or even edit the code - but in a sense its the opposite, because only an experienced web developer or at least a product manager would know how to instruct GPT in a useful manner.\n\n\\*\\*\\* We are seeking donations to ensure this project continues and, quite literally, keep the lights on. Cryptos, cash, cards, openai access tokens with free credits, hardware, cloud GPUs, etc... all is appreciated. Please DM to support this really cool open source project \\*\\*\\*\n\nPS. I'm the injured engineer who made this thing out of necessity, because i injured my wrist building an AI platform that's become way too big for one engineer to maintain. So AMA :)", "score": 159}, {"text": "[N] PyG 2.3.0 released: PyTorch 2.0 support, native sparse tensor support, explainability and accelerations. PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n\nToday version 2.3 got released: https://github.com/pyg-team/pytorch_geometric/releases/tag/2.3.0", "score": 40}, {"text": "[D] Ben Eysenbach, CMU: On designing simpler and more principled RL algorithms. Listen to the [podcast episode](https://generallyintelligent.com/podcast/2023-03-22-podcast-episode-30-ben-eysenbach/) with Ben Eysenbach from CMU where we discuss about designing simpler and more principled RL algorithms!", "score": 4}, {"text": "[N] Swiss DoD researchers published today a report on how they think LLMs will impact cyber-security and cyber defense. ", "score": 6}, {"text": "[R] Question about Selection of Machine Learning Type for a Neuroscience/Biomedical Engineering Problem. All:\n\nThank you for reading this. I have the following problem and goals:\n\nLet's say that I have measurements of neuron spiking activity from a particular location of the brain of a rat. This rat is also involved in a behavioral task in which the rat has to press a button in response to some visual cue. Suppose we have the following neuron spike activity time series with overlaid instances of button presses:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/t2tdfywr6hpa1.png?width=1890&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6f005593bb4749b1c7935ab667a8c2569b9a8a7b\n\nI want to identify feature of the time series (for example, in the frequency domain) to then use to make a model that can make predictions on button presses based on neuron spike activity. I'm under the impression that I can then arrive at confidence intervals for button presses (in terms of the time period window when the model 'thinks' a button press has occurred).\n\nI'm lost when it comes to types of machine learning models I can use for my particular goal. Any input is appreciated. If I need to provide more information, please let me know. Thank you again.", "score": 18}, {"text": "[R] Zero-shot Sign Pose Embedding model. We built a model that converts sign language videos into embeddings. It takes body and hand pose keypoints from a video and converts this into an embedding for use in downstream tasks. We show how classification can be done on an unseen dataset.\n\nYou can check out the repo at [https://github.com/xmartlabs/spoter-embeddings](https://github.com/xmartlabs/spoter-embeddings) and the accompanying blog post [here](https://blog.xmartlabs.com/blog/machine-learning-sign-language-recognition/).", "score": 9}, {"text": "[P] New toolchain to train robust spiking NNs for mixed-signal Neuromorphic chips. ", "score": 18}, {"text": "[D] Best decoder only Language model under 400M parameters ?. Hello,\nI\u2019m looking for a decent GPT-like Language model which is relatively small in size.\n\n Thanks in advance !", "score": 5}, {"text": "[P] Serge, a self-hosted app for running LLaMa models (Alpaca) entirely locally, no remote API needed.. Hello there!\n\n[Serge chat UI, with conversations on the left](https://preview.redd.it/rayrn7m4ncpa1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bea149c499f4f0b9ad0b6aceb9dc21404c6e9d5)\n\nI've recently been working on Serge, a self-hosted dockerized way of running LLaMa models with a decent UI &amp; stored conversations. It currently supports Alpaca 7B, 13B and 30B and we're working on integrating it with LangChain and the ReAct chain agent.\n\nI've tried my best at making the instructions dead easy, so it's all dockerized with a download manager for weights and it can be run with almost zero configuration required.\n\nI think being able to run those models locally will be key to expanding their ability, and so I hope this can contribute to that.\n\nLet me know if you have any feedback or suggestions on how to extend its capabilities!\n\n&amp;#x200B;\n\nGitHub: [https://github.com/nsarrazin/serge](https://github.com/nsarrazin/serge)", "score": 138}, {"text": "[D] Starting a discord server to teach people ML. Hey guys I\u2019m thinking of starting a discord server to help people learn machine learning.\n\nI was thinking of doing weekly live lessons where i teach machine learning concepts.\n\nA little bit of background about me, I\u2019m artificial intelligence engineer, currently pursuing a PhD in reinforcement learning. I have deep expertise in computer vision and LLMs.\n\nThe discord server is totally free. I'm not charging for this,\n\nIf you're interested in joining the server PM me and I'll send you an invite link", "score": 1}, {"text": "[R] Introducing SIFT: A New Family of Sparse Iso-FLOP Transformations to Improve the Accuracy of Computer Vision and Language Models. **Note #2:** We are revising the name to Sparse-IFT. We appreciate the candid feedback and look forward to hearing any additional feedback you have on our research.\n\n**Note**: Thank you r/MachineLearning for providing so many awesome naming alternatives! We'll revisit the acronym and update accordingly.\n\nWe are excited to announce the availability of our [paper on arxiv](https://arxiv.org/abs/2303.11525) on Sparse Iso-FLOP Transformations (Sparse-IFT), which increases accuracy and maintains the same FLOPs as the dense model using sparsity. In this research, we replace dense layers with Sparse-IFT and significantly improve computer vision and natural language processing tasks without modifying training hyperparameters\n\nSome of the highlights of this work include ResNet-18 on ImageNet achieving a 3.5% accuracy improvement and GPT-3 Small on WikiText-103 reducing perplexity by 0.4, both matching larger dense model variants that have 2x or more FLOPs.\n\nSparse-IFT is simple to use, provides a larger search space to find optimal sparse masks, and is parameterized by a single hyperparameter - the sparsity level.\n\nThis is independent of the research we [posted](https://www.reddit.com/r/MachineLearning/comments/11xskuk/r_spdf_sparse_pretraining_and_dense_finetuning/) yesterday, which demonstrates the ability to reduce pre-training FLOPs while maintaining accuracy on downstream tasks.\n\nThis is the first work (that we know of!) to demonstrate the use of sparsity for improving the accuracy of models via a set of sparse transformations.\n\nhttps://preview.redd.it/qznj00gex6qa1.jpg?width=3536&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2e3ee31dd58f76ab7e2c24105081574f772ed0b1", "score": 76}, {"text": "[D] LLaMA or Alpaca Weights. Was anyone able to download the LLaMA or Alpaca weights for the 7B, 13B and or 30B models? If yes please share, not looking for HF weights", "score": 15}, {"text": "[P] An online evaluation tool for binary classifiers. [https://modeleval.net/](https://modeleval.net/)\n\nI spend a lot of time iterating on classifiers and showing results to higher-ups, so started working on this tool which generates a linkable page with some basic metrics. Only binary classifiers for now, but will add support for multilabel classifiers if there is enough interest. Let me know if you find this useful.", "score": 3}, {"text": "[P] Open-source GPT4 &amp; LangChain Chatbot for large PDF docs. GitHub: [https://github.com/mayooear/gpt4-pdf-chatbot-langchain](https://github.com/mayooear/gpt4-pdf-chatbot-langchain)  \nDemo video: [https://www.youtube.com/watch?v=ih9PBGVVOO4](https://www.youtube.com/watch?v=ih9PBGVVOO4)", "score": 21}, {"text": "[N] Prompt-to-voice (Dall-E for Voice). Blogpost: [Introducing Prompt-to-Voice - Describe It to Hear It / Blog / Coqui](https://coqui.ai/blog/tts/prompt-to-voice)  \n\n\nThere is still space for improvement, but that is an exciting take on voice creation. \n\nI wonder if it'd be open-sourced alongside [TTS.](https://github.com/coqui-ai/TTS)", "score": 6}, {"text": "[D] Cost for serving realtime inferences of a model like PaLM 62B. Hi, I'm not that familiar with LLMS but I would like to know if it is reasonable to assume an approximation on the costs of serving a model like PaLM 62B in AWS considering an average of 100 tokens for input and 300 tokens for input per request and having about 10 thousand requests per month. I remember reading not long ago that the costs of serving GPT3 were token-based.", "score": 2}, {"text": "[P][R][D] Feature Subset Selection (NP-Hard). Hey guys, for my project this semester I am to tackle the problem of Feature Subset Selection.\n\nMy original approach to this problem was to find a pure categoric dataset to run classification tasks, a pure numeric one for both classification and regression and lastly multiple multivariate ones for both tasks.\n\nI will be using ITMO FS and ASU libraries and multiple of their different algorithms to find feature subsets from these databases.\n\nThe candidate features outputted from these algorithms will be used to train multiple ML models. The goal is not to rank the ML models but to rank or discuss the FS algorithms but ML is needed all throughout the way. An ensemble final prediction (or a score) will be taken from the ML predictors.\n\nThan comes the fitness measuring part where I will be ranking the selected feature subsets, trying to find common selected features accross the FS algorithms or any correlations, rules, anything interesting as the findings part tbh. I am planning on using this post as an update board a discussion board and a helpline.\n\nFor the start I need to find the datasets. I think that different datasets with both high and low number of features should be used. There comes the first part I need help with. Finding the candidate datasets. I am open to any recommendations of datasets to use, their numbers and everything really.\n\nAny help all throughout this campaign is highly appreciated.\n\nThanks in advance you awesome redditors", "score": 2}, {"text": "[D] Overwhelmed by fast advances in recent weeks. I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.\n\n&amp;#x200B;\n\nFirstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.\n\n&amp;#x200B;\n\nNot only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.\n\n&amp;#x200B;\n\nIn addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.\n\n&amp;#x200B;\n\nFor the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with \"new ideas, that set us apart\".\n\n&amp;#x200B;\n\nWatching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.\n\n&amp;#x200B;\n\nThe hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.\n\n&amp;#x200B;\n\nI can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.\n\n&amp;#x200B;\n\nAs Huang said in his keynote, companies want to develop \"disruptive products and business models\". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.\n\n&amp;#x200B;\n\nIn conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.\n\n&amp;#x200B;\n\nHow are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?", "score": 800}, {"text": "[R] Learning laws of physics via RL with units constraints. ", "score": 6}, {"text": "[N] [D] GitHub Copilot X Announced. Website: [https://github.com/features/preview/copilot-x](https://github.com/features/preview/copilot-x)  \nAnnouncement video: [https://www.youtube.com/watch?v=4RfD5JiXt3A](https://www.youtube.com/watch?v=4RfD5JiXt3A)\n\nWhat do you think?\n\nAlso, here are some other open-source GitHub projects and product integrations of GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). Feel free to contribute to that list.", "score": 106}, {"text": "[R] Kentucky Derby Race Tracking; featuring tracking by program number and position across 320 frames taken from open media to enable testing of predictive analytics, player performance tracking and improve betting sports. &amp;#x200B;\n\nhttps://preview.redd.it/jv9znuu37hpa1.png?width=1408&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=623b102ded74ba0a73cb0a3019c3c8eb99c38b38\n\n320 frames taken Kentucky Derby Highlights from 2019 to 2022 with horses being tracked per frame \u2013 identifying race succession. The dataset is appropriate for training computer vision- and data science-based predictions in horse races to complement betting systems.\n\nCheck it out on Kaggle: [https://www.kaggle.com/datasets/sadhliroomyprime/kentucky-derby-tracking](https://www.kaggle.com/datasets/sadhliroomyprime/kentucky-derby-tracking)\n\nThe source data was collected from open media, specifically from Kentucky Derby Highlights hosted on YouTube. We took the highlights from 2019 to 2022 where each year\u2019s race are organised in 80 frames (320 in total).\n\nIt is difficult to develop a dataset from 2D imageries in open media, especially that of tracking. Therefore, we adopted the consensus-based quality check model of Acme AI \u2013 placing three of our quality checkers to blind test the dataset to ensure quality is not compromised. For extreme opaque cases, we used an Unknown classifier.\n\nEach of the horse is tracked by its program number (the number each horse wears on its saddlecloth) and relative position as attributes. Program numbers are sourced from official records of the Derby races.\n\nWe used SuperAnnotate\u2019s vector editor to label and classify the images. Export was made in COCO with fused labels to optimise interoperability and visual understanding.\n\nDataset is created by Acme AI Ltd. ([www.acmeai.tech](https://acmeai.tech/)) and is #openaccess \ud83d\ude0a \ud83d\ude0a", "score": 2}, {"text": "[D][R] Concerns about using Conformer with Classification Token.  \n\nHello everyone,  \nI have a question regarding the combination of Conformers and Classification Tokens.   \nAs I know, Conformers are a variation of Transformers, with added convolutional layers, while Classification Tokens are special-purpose inputs used in models like BERT.   \nThese tokens are usually added to the beginning of sequence data to help identify the entire sequence.  \nIn the original BERT model, where Transformers are used, it seems that there is no issue in using a classification token.   \nHowever, I have concerns about how well it would work with a Conformer due to the presence of convolutional layers.  \nMy specific concern is that if the classification token is added to the beginning of the sequence, only the initial part of the sequence would be influenced by the classification token through the convolutional layer, leaving the latter parts unaffected.  \nDespite my concerns, I have seen research that combines Conformers and Classification Tokens.   \nI am wondering if there is actually no problem with this approach.   \nAlternatively, is there a way to circumvent this issue?   \nThank you in advance!", "score": 3}, {"text": "[D] LLMs\u2019 use of synthetic data. I recently did an \u201c[interview](https://www.tonic.ai/blog/how-bing-uses-synthetic-data-to-improve-its-models-as-explained-by-bing?utm_campaign=Blogs&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_term=r%2FMachineLearning&amp;utm_content=How)\u201d with Bing about its use of synthetic data in its training sets.\n\nIt talked about:\n\n* The definition of synthetic data and use cases\n* Its use of GANs to generate synthetic data when there isn\u2019t enough quality data for it to draw insights and patterns from\n* Methods for synthetic data generation\n\nI\u2019m interested to hear peoples\u2019 thoughts on LLMs generating their own synthetic data to add to their training sets. It described it itself as a bit of a feedback loop and I\u2019m curious to hear peoples\u2019 opinions on the dynamic of a model generating its own data to train on.", "score": 2}, {"text": "[D] [P] I asked GPT-4 to try &amp; dethrone the transformer. After some iterations, this is where it got to. I am not well versed in ML at all (understatement) &amp; did this out of curiosity. I have no way to judge it nor computational power to train it. Can anyone tell me whether it did a good job?. ", "score": 0}, {"text": "[D] ML model to find text/similar text in pdf. Hi all, \nI am trying to build a ML model that find occurrence of text/similar text in a pdf and returns a %match of that text I am looking for. \nTF-IDF looks like one of the models I can use. Does anyone know another model that might be useful for this? Maybe something that can produce reliable results after training on like 500-600 documents?", "score": 3}, {"text": "[D] Logistic weka in sklearn. My professor got some results using logistic modul in weka. He then asked me to implement this algorithm in python using sklern. Also he used gain ratio for filtering attributes. I'm having difficulty getting same results on same data. I'm not even sure witch implementation of logistics regression to use in sklern or if the mutal information in sklearn is the same as gain ratio in weka. I ended up using python weka wrapper just so I get the same results as he did. But I would like to know what was the real difference between logistic regression in sklearn and weka? And how to get gain ratio from weka using mutal information in sklearn?", "score": 2}, {"text": "[D] Question for use of ML in adaptive authentication. Hi all, I'm looking for advice for using ML for Adaptive Authentication.\n\nThe use case is that I want to generate a unique identifier key from user bahavior. eg: Sam uses my app and I want to generate key 1234, Mel uses the app, her key is 2351, etc\n\nTo generate this key I thought I could use an ML model that takes as input user behavior data and outputs this key or something I can use to derive a key.\n\nTaking typing on a smartphone as an example: a user types 10 words on their keyboard, we take data from that and feed it to the model to generate the key for this user. The data we take might be something like speed of typing a letter, time fingers were pressed on keys, number of times they used backspace, etc...\n\nIs this possible? I'm not an ML specialist so my knowledge is limited, but I was thinking we could do something like using a classifier with 10 categories, and use some statistical value from the output equivalent to prediction accuracy or prediction certainty for each category to generate numbers out of the classifications... but that seems like a hack and there may be something more precise and standard", "score": 2}, {"text": "[P] CleanVision: Audit your Image Data for better Computer Vision. To all my computer vision friends working on real-world applications with messy image data, I just open-sourced a Python library you may find useful!\n\n[Some issues detected in the Caltech-256 dataset.](https://preview.redd.it/smaldg3c5bpa1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b78a1d0aa64b669f9b546ad7321a650acc59f8a7)\n\nCleanVision audits any image dataset to automatically detect common issues such as images that are blurry, under/over-exposed, oddly sized, or near duplicates of others. It\u2019s just 3 lines of code to discover what issues lurk in your data before you dive into modeling, and CleanVision can be used for **any** image dataset \u2014 regardless of whether your task is image generation, classification, segmentation, object detection, etc.\n\n    from cleanvision.imagelab import Imagelab \n    imagelab = Imagelab(data_path=\"path_to_dataset\")\n    imagelab.find_issues()\n    imagelab.report()\n\nAs leaders like Andrew Ng and OpenAI have lately repeated: models can only be as good as the data they are trained on. Before diving into modeling, quickly run your images through CleanVision to make sure they are ok \u2014 it\u2019s super easy!\n\nGithub:  [https://github.com/cleanlab/cleanvision](https://github.com/cleanlab/cleanvision)", "score": 38}, {"text": "I made a tool that saves your ChatGPT conversations in .md file. And it's Open Source. [P]. [https://github.com/MatveyM11/Mine-ChatGPT](https://github.com/MatveyM11/Mine-ChatGPT)\n\nThe extension has already been submitted for approval in the Chrome Web Store.", "score": 5}, {"text": "[P] ChatLLaMA - A ChatGPT style chatbot for Facebook's LLaMA. \ud83d\udc4b  Hey all, we just launched [ChatLLaMA](https://chatllama.baseten.co/). An experimental chatbot interface for interacting with variants of Facebook's LLaMa. Currently, we support the 7 billion parameter variant that was fine-tuned on the Alpaca dataset. This early version isn't as conversational as we'd like, but over the next week or so, we're planning on adding support for the 30 billion parameter variant, another variant fine-tuned on LAION's OpenAssistant dataset and more as we explore what this model is capable of.\n\nIf you want deploy your own instance is the model powering the chatbot and build something similar we've open sourced the Truss here: [https://github.com/basetenlabs/alpaca-7b-truss](https://github.com/basetenlabs/alpaca-7b-truss)\n\nWe'd love to hear any feedback you have!\n\n[Check it out here](https://chatllama.baseten.co/)", "score": 27}, {"text": "GPT-4 For SQL Schema Generation + Unstructured Feature Extraction [D]. GPT-4 is out and I think data engineering is going to be out the door soon, I saw this post on medium recently: [https://medium.com/@nschairer/gpt-4-data-pipelines-transform-json-to-sql-schema-instantly-dfd62f6d1024](https://medium.com/@nschairer/gpt-4-data-pipelines-transform-json-to-sql-schema-instantly-dfd62f6d1024)\n\nAnd I was pretty amazed at how well GPT-4 can generate a SQL schema from raw JSON data, and had to wonder if we are wasting our time with NLP models for extracting information from raw text. For example, you could use bs4 to pull all inner text out of certain web forms and have GPT-4 extract meaningful information from them (say SEC filings with pseudo standard fields)...anyone agree?", "score": 12}, {"text": "[P] fastLLaMa, A python wrapper to run llama.cpp. Hi all, I have been working on fastLLaMa. It is a Python package that provides a Pythonic interface to a C++ library, llama.cpp. It allows you to use the functionality of the C++ library from within Python, without having to write C++ code or deal with low-level C++ APIs.\n\nUsing fastLLaMa, you can ingest the model with system prompts and then save the state of the model, Then later load the state, and start inferencing the model immediately.\n\nNo noticeable performance drop between lama.cpp and fastLLaMa.\n\nHave a look at it if it is of interest and do let me know what you think :)\n\n[Repo Link](https://github.com/PotatoSpudowski/fastLLaMa)\n\n[Tweet](https://twitter.com/Bahushruth/status/1638231265320239106)  \n\n\nUpdate 1:  \nHi good folks!  \nI have added support to export Peft fine-tuned models and run them as well.   \nShould be easy to quickly test out the capabilities of Peft fine-tuning.   \nHappy hacking :)", "score": 112}, {"text": "[D] ICML 2023 Reviewer-Author Discussion. Thought it might make sense to create a discussion thread specifically for reviewer-author discussion. It seems that many authors (at least those around me) did not receive any further response from the reviewers. How's everyone's discussion period going?\n\nPS: I understand that ICML reviewers are busy with their own research/work and are managing many submissions at the same time. I just wish they could be more active in the discussion period, because all these submissions are the results of many months of hard work. Personally I am also a reviewer at ICML and have responded to most authors.", "score": 23}, {"text": "[R] MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action.  Blog - [https://multimodal-react.github.io/](https://multimodal-react.github.io/)\n\nPaper - [https://arxiv.org/abs/2303.11381](https://arxiv.org/abs/2303.11381)\n\nCode - [https://github.com/microsoft/MM-REACT](https://github.com/microsoft/MM-REACT)\n\nDemo - [https://huggingface.co/spaces/microsoft-cognitive-service/mm-react](https://huggingface.co/spaces/microsoft-cognitive-service/mm-react)\n\nWildest thing i've seen in a while. Still processing how a connection of foundation models can be this good.", "score": 84}, {"text": "[D] can anyone suggest some Django framework tutorial???. I am new to ML. Lately for some ML task, I need to learn about Django framework. Can anybody suggest some tutorial???", "score": 0}, {"text": "[D] ICML rebuttal discussion stage. I have been distributed with four reviewers, three of which missed my contribution, they surely have a unfinished review. \n\nIn rebuttal stage, I answer all of the questions and weakness about technical. However, I have not got any replies.\n\n\n\nP.S I have requested area chair supervising them having another full review. But no any simple feedback submitted.\n\n\nHow to deal with this suitcase? My first time to ICML conference. So frustrated to the so-called openreview discussion stage.", "score": 6}, {"text": "Machine Learning for Materials[D]. Is this subfield growing ? Is it advisable to go for a full fledged phd in this subject.", "score": 5}, {"text": "[P] CodeAlpaca Code and Data release. Released the data and code used to train CodeAlpaca - [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)", "score": 11}, {"text": "[R] Prompting ChatGPT for visual math and text reasoning. &amp;#x200B;\n\nhttps://preview.redd.it/m7tdhkd2gbpa1.jpg?width=449&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7197a41640b06e15e1be78549303791d94dc7f0e", "score": 5}, {"text": "[D] NLP: Infer intent of finalising a transaction in a dialogue/chat system. Hi all,\n\nI have been tasked with tacking the following problem and I wanted to ask for different approaches on how to best approach it.\n\n**Problem**  \nI am looking to infer the intent of finalising the transaction during a chat conversation. For example: buyer messages \u201care there any scratches on the table?\u201d and gets a response \u201cno, there are no scratches, the table is brand new\u201d the probability of finalizing the transaction is 89%.\n\n**Data Available**  \nChat data is available for the last month all in **Polish** with a flag pointing if a transaction was completed or not. The feedback was acquired by sending a custom binary closed question 48h after the conversation ended probing both sides buyer and seller.\n\n**My approach**  \nI was looking to preprocess the whole dialogue (remove stopwords, lemmatisation) as one text and pass it through a TF-IDF (use n-grams as well). Then based on the frequency of words determine how relevant those words are to a transaction or not and then fit a classifier (naive bayes) to determine the probability of a transaction. An open question still to answer is to use the whole dialogue up until a point or just use the last 2,4\u2026 message exchanged between the buyer and the seller.\n\nLooking forward to your thoughts on the topic. Thanks a lot in advance for your help.", "score": 2}, {"text": "[D] What is your methodology for reviewing preprints?. In a similar vein to the recent posts on the rapid advances that have been occurring recently, I have started to see a noticeable increase in the number preprint papers posted to arXiv on the research topics that I am interested in. \n\nClearly, the signal to noise ratio is much higher with preprints as they have not been through the review process. This is why I am interested in looking at what everyone else do to deal with the large influx of preprint papers especially as conference season is about to kick into gear. \n\nDo you have any heuristic or rules of thumb in picking preprints to read? Do you wait till they have gone through the proper journal or conference review before reading them?", "score": 2}, {"text": "[P] Recommendation of one type of content based on another type of content. Hi, does anybody have any resources or know where to find any about recommending one type of content (for example movies) based on another type (for example books or articles) the user is currently reading?\n\nTheres a short summary written for the movies so I'm guessing I would have to map semantically similar books to movie summaries?\n\nAny resources would be of great help. Thanks in advance.", "score": 2}, {"text": "[Project] Q-learning from 2 to 3 dimensions. I am trying to implement a Q-learning pricing algorithm that can take a price from another company and from this price set the optimal price to maximize its profit. Right now it works fine for two companies, but after implementing it for three companies, the companies stopped setting the most optimal price and start to collaborate at a sub-optimal price. The implementations can be seen in the pictures. I simply just added an extra dimension to the Q-matrix, and add an extra price vector and state to compensate for the new company. For the New estimate of the Q value I just expanded it as seen in the picture by taking account of the two periods where the other algorithms set their prices. Is this too naive an approach? Is there any guidance out there on how to expand a Q-function to more dimensions?\n\nhttps://preview.redd.it/b2t4roqe4apa1.png?width=1006&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03068ffa1c32bafec148d49b468e38c8cd06d33e", "score": 3}, {"text": "[D] Overview of advancements in Graph Neural Networks. Hi all,\n\nRecently I\u2019ve been getting up to speed on **Graph Neural Networks** (GNN) and the results of applying them vs. other methods.\n\nI\u2019ve found that GNN approaches made impressive strides and have led to some really substantial performance jumps in actual production models in the industry. A new GNN-based model for estimating the time of arrival within Google Maps (with accuracy **improvements up to 50%**) is just one of many interesting examples.\n\nThis pushed me to summarize my findings and write up a blog post on the **state of Graph Neural Networks in 2023**. It provides an overview of the recent advancements that GNNs have made in industry, as well as a couple of impressive statistics about their current place in the AI research landscape.\n\nI plan to write more articles on GNNs, like a short series that will dive into technical details of how GNNs work and more, so if you enjoy this article please let me know so I can be sure to develop the rest of the series and publish soon!\n\nWould be very helpful to hear your thoughts on this.", "score": 157}, {"text": "[R] SPDF - Sparse Pre-training and Dense Fine-tuning for Large Language Models. Hey everyone!\n\nCerebras is excited to share that our sparsity paper is now available on [arxiv](https://arxiv.org/abs/2303.10464) and has been accepted into the ICLR 2023 Sparsity in Neural Networks [workshop](https://www.sparseneural.net/home)!\n\nThis research demonstrates the ability to pre-train large GPT models with high levels of sparsity followed by dense fine-tuning to maintain accuracy on downstream tasks.\n\nWe achieved this using Cerebras CS-2, a system that accelerates unstructured sparsity and allows exploration of machine learning techniques at a larger scale than previously possible.\n\nThe researchers used simple, static sparsity and evaluated model sizes up to GPT-3 XL with 1.3B parameters. We were able to pre-train GPT-3 XL with up to 75% unstructured sparsity, and 60% fewer training FLOPS on Cerebras CS-2. These findings show the promise of sparse training and motivate exploration of more advanced sparse techniques for even larger models.\n\nThis is the first time a large GPT model has been pre-trained with high sparsity without significant loss in downstream task metrics, and the results are exciting for the industry as it offers a fundamental enabler to reduce the compute to train these models.", "score": 50}, {"text": "[Project] Machine Learning for Audio: A library for audio analysis, feature extraction, etc. **audioflux** is a deep learning tool library for audio and music analysis, feature extraction. It supports dozens of time-frequency analysis transformation methods and hundreds of corresponding time-domain and frequency-domain feature combinations. It can be provided to deep learning networks for training, and is used to study various tasks in the audio field such as Classification, Separation, Music Information Retrieval(MIR) and ASR etc.\n\n**Source Code**: [https://github.com/libAudioFlux/audioFlux](https://github.com/libAudioFlux/audioFlux)", "score": 251}, {"text": "[D] Do you have a free and unlimited chat that specializes only in teaching programming or computing in general?. I'm seeing the various attempts, all valid and very welcome, to create general conversation chats at the level of ChatGPT 4.0 or similar.\n\nBut I would find it very helpful if some of the current attempts to make a general conversation chat at the level of GPT 4, were a specific conversation chat (weak artificial intelligence, but strong - term coined by me now) that did everything that ChagGPT 4.0 in the area of informatics, mainly programming.\n\nThat is, instead of having a 7B model, for example, dedicated to general conversation that works more or less, we would have a 7B specifically designed just for computing, allowing each of us to have a private computing teacher on our own PC who teaches us and writes code for people if asked, but a teacher who \"knows everything\" about computers.\n\nI'm doing a postgraduate course in Artificial Intelligence Engineering and I'm starting to enter the world, there are a lot of things I have to know. If I knew and had equipment for this, I would create a model just for this purpose.", "score": 2}, {"text": "[D] Running an LLM on \"low\" compute power machines?. It's understandable that companies like OpenAI would want to charge for access to their projects due to the ongoing cost to train then run them, I assume most other projects that require as much power and have to run in the cloud will do the same.\n\nI was wondering if there were any projects to run/train some kind of language model/AI chatbot on consumer hardware (like a single GPU)? I heard that since Facebook's LLama leaked people managed to get it running on even hardware like an rpi, albeit slowly, I'm not asking to link to leaked data but if there are any projects attempting to achieve a goal like running locally on consumer hardware.", "score": 49}, {"text": "[P] Run LLaMA LLM chatbots on any cloud with one click. We made a \\*basic\\* chatbot based on LLaMA models; code here: [https://github.com/skypilot-org/skypilot/tree/master/examples/llama-llm-chatbots](https://github.com/skypilot-org/skypilot/tree/master/examples/llama-llm-chatbots) [https://github.com/skypilot-org/sky-llama](https://github.com/skypilot-org/sky-llama)\n\nA detailed post on how to run it on the cloud (Lambda Cloud, AWS, GCP, Azure) with 1 command: [https://blog.skypilot.co/llama-llm-chatbots-on-any-cloud/](https://blog.skypilot.co/llama-llm-chatbots-on-any-cloud/)\n\nWould love to hear your thoughts. Although people are making LLMs run on laptops and other devices ({llama,alpaca}.cpp}, we think that as more open and compute-hungry LLMs emerge, it's increasingly important to finetune them and that's where getting powerful cloud compute in flexible locations comes into play.", "score": 26}, {"text": "[D] Information about the International Conference on Neural Information Processing (ICONIP). Greetings to all,\n\nI am curious to know if any of you have had the experience of publishing at ICONIP. I would greatly appreciate any insights you may have about the review process, the level of difficulty, and the overall quality of the review process. \n\nHere's the link for the 2023 edition, the paper submission deadline is on June 10th: [http://iconip2023.org/](http://iconip2023.org/)\n\nThank you in advance for your valuable input.", "score": 2}, {"text": "[d] combined image/text dataset. Hi,\n\nIs any dataset that contains images and texts in a sequential format available? LAION-5B only contains image/text pairs, and The Pile only includes the text. I would like to know if there is any existing dataset like The Pile but with images inside it. Like in case of news article, all the images at their right place should be included.\n\nBest regards", "score": 2}, {"text": "Zero-1-to-3: Zero-shot One Image to 3D Object. ", "score": 69}, {"text": "[D] [P] Curating open-source projects and community demos around GPT-4. There are many open-source projects and indie-built demos around the GPT-4 API. Despite the recent shift of OpenAI toward closure, open demos are always advancing the field and inspiring creativity. Here are some community projects that I find particularly interesting: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). Feel free to share the things you've been building or something you've been fascinated about on social media either by joining the discussion here or by contributing to the repository:)", "score": 8}, {"text": "[D] Is ML doomed to end up closed-source?. So basically, OpenAI is keeping its models a secret, Hugging Face added a new gated feature, and LLaMA is using a non-commercial license. It looks like companies are all moving towards closed-source and monopolizing ML. \n\nI've always loved Hugging Face, but now they are doing the opposite of what they preach with this new gated feature thing, this is just not open-source and shouldn't be encouraged in the first place.\n\nOpen AI [clearly stated](https://openai.com/policies/terms-of-use#:~:text=use%20output%20from%20the%20Services%20to%20develop%20models%20that%20compete%20with%20OpenAI) that you can't \"use output from the Services to develop models that compete with OpenAI\"\n\nGoogle shared its paper Attention Is All You Need transparently which was a breakthrough in NLP and got utilized by OpenAI (with many other papers) to build GPT-4 which is adopted by Bing and now posing risk to Google's business. As a consequence, could companies start to avoid sharing research openly and rather monopolize their work for the sake of their own business safety?\n\nAlso, assuming we will witness more of these closed-source models. is it safe to just trust them without understanding what data they got exactly trained on? This doesn't seem to make sense, not sure how this would end up.", "score": 613}, {"text": "[R] Comparison of XAI libraries for Computer Vision. I am working on an open-source XAI library aggregator and made a comparison of currently active XAI repositories. I am sure somebody will find this useful :)\n\n| Feature                              | pytorch-grad-cam | Captum | pytorch-cnn-visualizations | torch-cam | innvestigate | tf-explain | OmniXAI | Xplique | M3d-Cam |\n|--------------------------------------|------------------|--------|----------------------------|-----------|--------------|------------|---------|---------|---------|\n| Repository has library API           | \u2714                | \u2714      | \u2718                          | \u2714         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Contains tutorials or examples       | \u2714                | \u2714      | \u2714                          | \u2714         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Documentation contains API reference | \u2718                | \u2714      | \u2718                          | \u2714         | \u2714            | \u2714\u2718         | \u2714       | \u2714       | \u2714       |\n| Works with 2D images                 | \u2714                | \u2714      | \u2714                          | \u2714         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Works with 3D images                 | \u2718                | \u2718      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2718       | \u2714       |\n| Activation visualization             | \u2714                | \u2714      | \u2714                          | \u2714         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Overlaying  heatmap visualization    | \u2714                | \u2714      | \u2714                          | \u2714         | \u2718            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Explain image classification         | \u2714                | \u2714      | \u2714                          | \u2714         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Explain object detection             | \u2714                | \u2714      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2718       | \u2718       |\n| Explain semantic segmentation        | \u2714                | \u2718      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2718       | \u2714       |\n| Explain panoptic segmentation        | \u2718                | \u2718      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2718       | \u2718       |\n| GPU acceleration                     | \u2714                | \u2714      | \u2714                          | \u2714         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Trust metrics                        | \u2714                | \u2714      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2714       | \u2718       |\n| PyTorch support                      | \u2714                | \u2714      | \u2714                          | \u2714         | \u2718            | \u2718          | \u2714       | \u2718       | \u2714       |\n| TensorFlow support                   | \u2718                | \u2718      | \u2718                          | \u2718         | \u2714            | \u2714          | \u2714       | \u2714       | \u2718       |\n| Concept Activation Vector            | \u2718                | \u2714      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2714       | \u2718       |\n| Feature visualization                | \u2718                | \u2718      | \u2714                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2714       | \u2718       |\n| DeepDream                            | \u2718                | \u2718      | \u2714                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2718       | \u2718       |\n| Influential Examples                 | \u2718                | \u2714      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2718       | \u2718       | \u2718       |\n| Interactive explainer app            | \u2718                | \u2714      | \u2718                          | \u2714         | \u2718            | \u2718          | \u2714       | \u2718       | \u2718       |\n| Adversarial attacks                  | \u2718                | \u2714      | \u2718                          | \u2718         | \u2718            | \u2718          | \u2714       | \u2718       | \u2718       |\n| Model\u2019s attributes                   | \u2714                | \u2714      | \u2714                          | \u2718         | \u2714            | \u2714          | \u2714       | \u2714       | \u2714       |\n| Layer\u2019s attributes                   | \u2714                | \u2714      | \u2714                          | \u2714         | \u2714            | \u2718          | \u2714       | \u2714       | \u2714       |\n| Neuron\u2019s attributes                  | \u2718                | \u2714      | \u2718                          | \u2718         | \u2714            | \u2718          | \u2718       | \u2714       | \u2718       |\n| Integration with experiment tracker  | \u2718                | \u2718      | \u2718                          | \u2718         | \u2718            | \u2714          | \u2718       | \u2718       | \u2718       |", "score": 23}, {"text": "[D] Is attention ALIBI Attention with Linear Biases implemented in both decoder and encoder?. I've read the paper, yet I cannot seem to find an explicit statement of where in the transformers architecture the ALIBI mask is implemented.\n\nThanks", "score": 3}, {"text": "[Project] AI Voice Narrated Audiobooks. &amp;#x200B;\n\nDear friends, I have published The Art of the Deal narrated by the author himself, Donald J. Trump on YouTube. what other books should I do this with? and to be narrated by who? I have created a poll on YouTube aswell, please voice your insightful opinions they mean a great deal to my work\n\n[https://youtu.be/M8sll6XKJOw](https://youtu.be/M8sll6XKJOw)", "score": 0}, {"text": "[D] Does label distribution affect models performance, especially for deep learning models?. Currently I'm working on a project where the dataset are quite imbalanced (most and least class can be 10 times in size). The data and labels of the project are actually a time series, so naturally, when I'm splitting the samples according to time, the number of labels of different class are quite different. For example, in validation set, number of samples in class 2 can be 10 times of class 1, but only 5 times more in testing set. My supervisor asked me to make every sets to have the same label distribution (e.g. class 2 should always have roughly 5 times of class 1).\n\nI'm quite frustrated by this argument. Not only this makes cross-validation very annoying as it means every splits I must readjust the label distribution quite bruteforcely; this redistributing also meant that I will have to alter the testing set which in my undestanding is quite a big no-no; (and also the fact that I can't really turn down this argument as the problem does exist and I couldn't provide a better solution).\n\nSo I want to know do people actually re-adjust the number of labels just to make different set of data to have similar ratio in number of labels? Because I can't quite find any readings regarding such problems. In my current understanding, a good loss should make the models to \"immune\" to difference in label distribution. In the project, we have used label-distribution-aware margin (LDAM) loss. But in the paper, the author only experiments with imbalanced data.", "score": 8}, {"text": "[Project] Alpaca-30B: Facebook's 30b parameter LLaMa fine-tuned on the Alpaca dataset. How to fine-tune Facebooks 30 billion parameter LLaMa on the Alpaca data set.\n\nBlog post: [https://abuqader.substack.com/p/releasing-alpaca-30b](https://abuqader.substack.com/p/releasing-alpaca-30b)\n\nWeights: [https://huggingface.co/baseten/alpaca-30b](https://huggingface.co/baseten/alpaca-30b)", "score": 288}, {"text": "[p] detrex v0.3.0: A Major Update to Our Detection Transformer Codebase!. Happy to release our **detrex v0.3.0**, after two months, there're **lots of new algorithms**, **baselines and useful tools** have been supported in detrex. We will introduce them later.\n\nHere's our github link: https://github.com/IDEA-Research/detrex\n\nIf our repo can help you for your research, please consider give us a star~\n\n## What's New\n### New Algorithms and Pre-Trained Models\nThe **newly supported algorithms** in detrex v0.3.0:\n- PnP-DETR (ICCV' 2021)\n- Anchor-DETR (AAAI' 2022)\n- DETA (ArXiv' 2022)\n- MaskDINO (CVPR' 2023)\n\nAs for now, our detrex **support 14 detection transformer algorithms**.\n\nAnd after lots of experiments, we release a set of new baselines, which **are all better than their official implementation**\n\n| Model | Box AP (detrex) | Box AP (official) |\n|:---|:---:|:---:|\n| DETA-R50 | 50.2 **(+0.1)** | 50.1 |\n| H-Deformable-DETR-R50 | 49.1 **(+0.4)** | 48.7 |\n| DINO-R50 | 49.4 **(+0.4)** | 49.0 |\n\nAnd we also release **more than 30+** pretrained weights (including the converted weights) for the community research. All the pretrained model can be downloaded from our [Model Zoo](https://detrex.readthedocs.io/en/latest/tutorials/Model_Zoo.html)\n\n**DINO detector with ViT backbone**\n\n| Model  | Lr Sched | Box AP |\n|:---|:---:|:---:|\n| DINO-ViTDet-Base (4scale) | 12 | 50.2 |\n| DINO-ViTDet-Base (4scale) | 50 | 55.0 |\n| DINO-ViTDet-Large (4scale) | 12 | 52.9 |\n| DINO-ViTDet-Large (4scale) | 50 | 57.5 |\n\n**DINO detector with strong FocalNet backbone**\n\n| Model  | Lr Sched | Box AP |\n|:---|:---:|:---:|\n| DINO-FocalNet-Large-3level (4scale) | 12 | 57.5 |\n| DINO-FocalNet-Large-4level (4scale) | 12 | 58.0 |\n| DINO-FocalNet-Large-4level (5scale) | 12 | 58.5 |\n\n### New Training Techniques\n- Supported **Mixed Precision Training** by setting `train.amp.enabled=True`: **reduce 20% to 30% GPU memory usage**.\n- Supported **EMAHook** by setting `train.model_ema.enabled=True`: which can further enhance the model performance.\n\n| Model | Lr Sched | Box AP |\n|:---|:---:|:---:|\n| DINO-R50 w/o EMA | 12 | 49.0 |\n| DINO-R50 with EMA | 12 | 49.4 **(+0.4)** |\n\n- Supported **Encoder-Decoder Gradient Checkpoint** in DINO detector which can further reduce 30% GPU memory usage.\n- Support wandb logger\n- Support a great slurm training scripts", "score": 8}, {"text": "[P] OpenAssistant is now live on reddit (Open Source ChatGPT alternative). OpenAssistant bot is live on /r/ask_open_assistant. There are some limitations to the reddit bot; you can also try on the model in chat mode at https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming. Model is available for free download at https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b.\n\n\nPrompt it by creating a new text post (responds to text body of post), starting a comment with !OpenAssistant, or by replying directly to it. \n\nI have recently enabled memory for the bot so it should do a (pretty mediocre) job of continuing a conversation with you.", "score": 200}, {"text": "[D] what stories or literature best illustrate the importance of choosing the right objective function?. I don\u2019t mean L1 or L2 error but more capturing what matters most. \n\nI\u2019m trying to cite sources and preparing for a few presentations where we test different target variables that are forms of the same variable but some are regressed easier than others due to how they are normalized, transformed, relative to the time of prediction, or seasonality removed. \n\nI\u2019m looking to find illustrative stories and literature that describe the importance of carefully selecting the target variable or an intermediate in open-ended problem solving. Thanks for any thoughts.", "score": 1}, {"text": "[D] Implementation of NER Model with relationship extraction?. I want to train a model, which can extract information from a perfume description\n\nWhat it should extract are the perfume notes and the type (top, heart, base notes) related to the note", "score": 0}, {"text": "[D] Largest available encoder-only model for text?. We are witnessing a lot of progress on the encoder-decoder or decoder only settings. Any progress on encoder-only model such as BERT ? Is BERT-large the only publicly available largest encoder only model that is widely tested ?\n\n&amp;#x200B;\n\nEdit: I'm aware of Megatron BERT by NVIDIA that was supposedly a 3B parameter encoder model but that doesn't seem to be publicly available or heavily cited. Not sure how reliable that is.", "score": 3}, {"text": "[R] Data Annotation &amp; Data Labeling with AI.  I'm becoming more and more interested in the Data/Machine Learning space. I'm looking to create a startup in the data space.\n\nIt can be pretty hard to find the exact answers that you're looking for, so I decided to take my question to reddit to get an exact answer.\n\n**3 Questions:**\n\n1. Is there a model or machine learning technology that can replace the need for humans in data annotation and data labeling?\n2. What exactly does [Scale.ai](https://scale.ai/) do? What are their flaws? What gaps are they not filling?\n3. What are the best ways/sources to learn this subject? *Currently, I'm reading a ton of content on medium, but I'm sure there are better sources out there.*", "score": 0}, {"text": "[R] CodeAlpaca - Instruction following model to generate code. Finetuned LLaMA following methodology from Alpaca on code generation instructions.Demo - [https://code-alpaca-demo.vercel.app/](https://code-alpaca-demo.vercel.app/)\n\nWorking on open sourcing the code and data.\n\nUpdate - Repo and data now live - [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca) , would appreciate any feedback or improvements.  \n\n\nUpdate - Web demo now running 13B model trained on 20k samples.", "score": 139}, {"text": "CoLT5: Faster Long-Range Transformers with Conditional Computation. ", "score": 53}, {"text": "SmartyGPT: now with ChatGPT and GPT4 [P]. I want to announce that we have released v1.1.0 which includes access for ChatGPT and GPT4 for Plus suscribers! :)  [https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)", "score": 0}, {"text": "[D] fx= (1/sigma\u221a2\u03c0)*e^(-(x-u)^2/2sigma^2). How is this formula to calculate likelihood used in logistic regression to find the maximum likelihood ? Because I have seen videos where people calculated the likelihood by converting log(odds) to probabilities using the sigmoid function. Is this formula used in anyway to find the likelihood in logistic regression.", "score": 0}, {"text": "[R] \ud83e\udd16\ud83c\udf1f Unlock the Power of Personal AI: Introducing ChatLLaMA, Your Custom Personal Assistant! \ud83d\ude80\ud83d\udcac. \ud83d\ude80 Introducing ChatLLaMA: Your Personal AI Assistant Powered by LoRA! \ud83e\udd16\n\n&amp;#x200B;\n\nHey AI enthusiasts! \ud83c\udf1f We're excited to announce that you can now create custom personal assistants that run directly on your GPUs!\n\n&amp;#x200B;\n\nChatLLaMA utilizes LoRA, trained on Anthropic's HH dataset, to model seamless conversations between an AI assistant and users.\n\n&amp;#x200B;\n\nPlus, the RLHF version of LoRA is coming soon! \ud83d\udd25\n\n&amp;#x200B;\n\n\ud83d\udc49 Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)\n\n&amp;#x200B;\n\n\ud83d\udcda Know any high-quality dialogue-style datasets? Share them with us, and we'll train ChatLLaMA on them!\n\n&amp;#x200B;\n\n\ud83c\udf10 ChatLLaMA is currently available for 30B and 13B models, and the 7B version.\n\n&amp;#x200B;\n\n\ud83d\udd14 Want to stay in the loop for new ChatLLaMA updates? Grab the FREE \\[gumroad link\\]([https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)) to sign up and access a collection of links, tutorials, and guides on running the model, merging weights, and more.  (Guides on running and training the model coming soon)\n\n&amp;#x200B;\n\n\ud83e\udd14 Have questions or need help setting up ChatLLaMA? Drop a comment or DM us, and we'll be more than happy to help you out! \ud83d\udcac\n\n&amp;#x200B;\n\nLet's revolutionize AI-assisted conversations together! \ud83c\udf1f\n\n&amp;#x200B;\n\n\\*Disclaimer: trained for research, no foundation model weights, and the post was ran through gpt4 to make it more coherent.\n\n&amp;#x200B;\n\n\ud83d\udc49 Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)\n\n&amp;#x200B;\n\n\\*Edit: [https://github.com/serp-ai/LLaMA-8bit-LoRA](https://github.com/serp-ai/LLaMA-8bit-LoRA) &lt;- training repo/instructions (If anything is unclear just let us know and we will try to help/fix the issue!)  (Sorry for spamming the link, don't really know how else to remind people lol)", "score": 713}, {"text": "[P] Make AI Robust and Trustworthy with CAPSA!. Modern AI models show great potential across various applications, but their deployment in everyday life is limited due to a lack of trustworthiness. While accuracy is crucial, AI models must also recognize when they can and cannot be trusted to make decisions, especially in safety-critical systems. To bridge this gap, it\u2019s essential to develop AI models with built-in trust mechanisms for reliable decision-making in real-world scenarios.\n\nTrustworthiness in AI models can be improved by addressing three risk sources: Representation Bias, Epistemic Uncertainty, and Aleatoric Uncertainty.\n\n&amp;#x200B;\n\n* Representation Bias refers to the potential for the model to favor certain groups or types of data over others, leading to inaccuracies in its predictions with under-represented data.\n* Epistemic Uncertainty, also known as Model Uncertainty, describes the uncertainty associated with the model\u2019s ability to make accurate predictions based on the data it has been trained on. Epistemic uncertainty can be improved by training the model longer, or picking a model architecture with higher predictive capacity.\n* Aleatoric Uncertainty, also known as Data Uncertainty, refers to the inherent noise or unpredictability in the data itself. This type of uncertainty can arise due to factors such as measurement errors, labeling errors, or natural variations in the data. This can only be improved by improving the data source, or manually fixing the inherent issues that lie within the dataset.\n\n&amp;#x200B;\n\nTo address this issue of AI trust and gain knowledge of the risk metrics mentioned above, we are open-sourcing CAPSA -- a tool that automates the creation of robust and trustworthy neural networks! It is a Python library that utilizes wrappers to make tensorflow/keras models risk-aware. These wrappers work by augmenting a given model to support the risk metric the wrapper provides. The wrapped model gains risk awareness capabilities, outputting risk metrics mentioned above alongside its predictions. Since these wrapped models are simply augmented models, they can be further trained with Keras API.\n\n[How Representation Bias, Epistemic Uncertainty, and Aleatoric Uncertainty looks in regression and classification tasks with 2d and 1d datasets. CAPSA wraps your Keras models to output these risk metrics alongside of your model's prediction.](https://preview.redd.it/qi94awk1qxoa1.png?width=2756&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=cc1f05011ae7e54e653f06d4d544f315d6c17cbc)\n\nCheckout [CAPSA on Github](https://github.com/themis-ai/capsa) and STAR our repo if you find it cool or helpful for your projects!\n\nWe also have a [paper published](https://themisai.io/papers/capsa.pdf) if you'd like to learn more about the details of how some of our wrappers work.\n\nLet us know what other features you would like CAPSA to support and we'll work on adding them as well!", "score": 14}, {"text": "ICML rebuttals still not visible to reviewers? [D]. The rebuttals for ICML submissions were supposed to become visible to reviewers at 3pm ET on Sunday, with the author-reviewer discussion period beginning today at 10am ET, but OpenReview says my rebuttals are not visible to the reviewers yet. Is anyone else having this problem? Am worried I somehow made them only visible to the chairs", "score": 22}, {"text": "[R] First open source text to video 1.7 billion parameter diffusion model is out. ", "score": 1192}, {"text": "[News] Prompt engineering blog from OpenAI applied AI head. ", "score": 87}, {"text": "[D] Hyperparameter robustness of RL algorithms. \n\nI've gone through a lot of RL algorithms recently and a lot of them seem to be very sensitive to hyperparameters with performances varying by degrees of +/-10 in some cases in a scale of  100. Do reviewers consider them as limitations when evaluating these algorithms and yet they still get published, what's the way forward in the field of RL to reduce this?", "score": 6}, {"text": "[D] Determining quality of training images with some metrics. Hello ML sub,\n\nHow does one evaluate the quality of training images before actually training a model ? Training a model is surely expensive. What if one had a way of sort of ascertaining that the image quality of a training set for a particular task (say object detection or semantic segmentation etc) ? It doesn't have to be perfect but some kind of hint...\n\nCould you please point me to some papers or studies or discussions on this ?\n\nThere are some objective metrics like PSNR or SSIM but they need a reference image", "score": 4}, {"text": "[D] Stable Diffusion + 3D model texture generate. Transform the way you create textures for your 3D models with our AI-based texture generator plugin for Stable Diffusion! Support our Kickstarter campaign now and be among the first to experience this game-changing software.\n\n[https://www.kickstarter.com/projects/cyberomance/3diffusetexai-ai-powered-texture-generator-for-3d-models?ref=project\\_build](https://www.kickstarter.com/projects/cyberomance/3diffusetexai-ai-powered-texture-generator-for-3d-models?ref=project_build)", "score": 0}, {"text": "[R][D] Papers on Transductive Learning. Hi all,\n\nI'm trying to find some good papers on transductive learning. I'm looking for newly published ones in general however papers which aren't that recent but had a good impact would also be nice to read. I've been searching for some papers however I do not want to miss out on the really good ones. So could anyone suggest papers on transductive learning which you think that I should not miss out?\n\nAlso I'm not sure if this is the right subreddit for this but, there is something which I'm struggling with recently. I have to conduct my literature review but it's too difficult really. And it takes too long to understand an article. Do you guys also have some suggestions on how I could read an article more efficiently so that I could read multiple articles in a single day?", "score": 4}, {"text": "[P] Action Recognition in Computer Vision. Action recognition is a difficult task. Mainly because of its vagueness. Type of actions, duration, ontology, etc. I made a complete overview of the problem relevant today. Here is a collection of approaches, networks, and problem statements. It will help you clarify the task the next time you meet it.    https://medium.com/@zlodeibaal/action-recognition-in-the-wild-9eb7f12b4d12", "score": 10}, {"text": "[D]: Vanishing Gradients and Resnets. I am working with Resnets consisting of feedforward networks. Additionally, I am using Kaiming-He weight initialisation and ReLU as an activation function. Extending the network to more than 10 layers leads to vanishing gradients. I cannot use batch normalization because that would violate assumptions of a gradient penalty. What should I do? Should I form residual connections over longer steps?\nShould I implement artificial derivatives? What's the common remedy here?", "score": 3}, {"text": "IJCAI 2023 Reviews discussion [D]. This is my first time submitting to IJCAI. Any comments on how to respond to the reviews are welcome. Any help is appreciated.", "score": 2}, {"text": "[D] Best ChatBot that can be run locally?. What do you guys think is currently the best ChatBot that you can download and run offline? After hearing that Alpaca has results similar to GPT-3, I was curious if anything else competes.", "score": 18}, {"text": "[D] IJCAI 2023 Rebuttal Discussion. Title", "score": 16}, {"text": "[P] searchGPT - a bing-like LLM-based Grounded Search Engine (with Demo, github). ", "score": 228}, {"text": "Smarty-GPT: wrapper of prompts/contexts [P]. This is a simple wrapper that introduces any imaginable complex context to each question submitted to Open AI API. The main goal is to enhance the accuracy obtained in its answers in a **TRANSPARENT** way to end users.\n\n[https://github.com/citiususc/Smarty-GPT](https://github.com/citiususc/Smarty-GPT)", "score": 2}, {"text": "[R] What are the current must-read papers representing the state of the art in machine learning research?. Recently, John Carmack [suggested](https://twitter.com/ID_AA_Carmack/status/1622673143469858816) the creation of a \"canonical list of references from a leading figure,\" referring to a never-released reading list given to him by Ilya Sutskever.\n\nWhile there may be an undue interest in that specific list, MLR is such a big field that it's difficult to know where to start. What are the major papers that are relevant to state of the art work being done in 2023? Perhaps we may crowd-source a list here?", "score": 48}, {"text": "[D] For those who have worked 5+ years in the field, what are you up to now?. Hey,I've  been working in ML for the last 5-10 years, almost since deep learning  came up, mostly startups with various successes, sometimes doing more research sometimes doing more engineering tasks.\n\nThis year I was excited to manage a team focused on ML but plans have just changed in my company and this isn't going to happen anytime soon. I am interviewing for another company for a \"senior\"  role but they still ask me to do this basic ML assignment that takes a  few hours to complete. I have just realised how boring it became to me  to the point I don't even want to do it as I literally have done this  for the last 5+ years.\n\nFor those  who have been in the field for at least 5+ years, what are you up to now? Are you still doing ML? Have you moved into management? Have you started your own company? Moved to a different subfield perhaps?\n\nPS: I  have a PhD, in addition to those years of experience i mention. I am  aware this isn't related to ML purely but more like mid-career crisis,  but would be interested to know how people in the field have dealt with  it.", "score": 22}, {"text": "[Discussion] In which way could Machine Learning be useful for a journaling app?. As per Title, I would like to use Machine Learning to make the journaling expierence better. I got some Questions in that Regard.\n\nFirstly, is it meaningful to host the model on the users device, to keep the Data safe?\n\nWhat do you suggest would be a useful way? A chat that response to the entries? Or meaningful prompts?  \n\nShould the Model learn only from what the user has written in his journal or be pretrained of scientific data or other data to respond to what the user has written accordingly?", "score": 0}, {"text": "[D] Systematised Network Diagrams. Hi all, I've been working on this neural network diagram convention for a few years now and would love to hear your feedback on it. I have personally found it useful, so I thought I would share it in case others would like to adopt it too.\n\nMy motivation was that almost every scientific paper uses a different diagrammatic system to depict their networks. This puts a burden on the reader to get to grips with the unique system, compounded with understanding the novel network displayed.\n\nThe aim of this system is to be modular, minimalistic, and consistent, enabling fast interpretation and universal communication of network architectures in scientific papers and presentations. This outlined key system is designed to represent clear, symbolic placeholders for mathematical functions, much like Feynman diagrams for quantum field theory or logic gates for mathematical logic. My [GitHub](https://github.com/GeorgeBird1/Diagramatic-Neural-Networks) page on this system offers an easy way to centralise, date, and update the convention. No accreditation is required when using this system for any purpose.\n\nIt is easy for hand drawing, with an included shorthand notation, alongside a more technical depiction for use in papers. All symbols are constructable using common flow-chart shapes for easy implementation.\n\nHere are some example networks I've drawn using this system:\n\n[Depiction of various types of neural network architectures using this system.](https://preview.redd.it/2geqegs3cqoa1.png?width=928&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=669c19ece3bc82fd2189f9e325da282a51b70ca5)\n\nHere are the basic key-components, more can be found on my [GitHub](https://github.com/GeorgeBird1/Diagramatic-Neural-Networks) link:\n\n[The basic key system for constructing diagrams. More available on the GitHub](https://preview.redd.it/mlhxhgfbcqoa1.png?width=750&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5090b1a914dcadd8b491506fb469534e1437531f)\n\nThanks for reading, any feedback gratefully received! :)", "score": 14}, {"text": "[Project] What if FastAPI supported NumPy arrays and Pillow images?. When deploying ML models with FastAPI we always had to write our own serialisation code for numpy.ndarray and PIL.Image. Not only have we replaced FastAPI with up to 100x faster C-level library a couple of weeks ago, but we have also recently added support for all the fancy Pythonic types on both client and server sides.  \n\n\n[Check it out on GitHub/Unum-Cloud/UJRPC](https://github.com/unum-cloud/ujrpc#more-functionality-than-fastapi)  \n\n\nhttps://preview.redd.it/3m73l6qodpoa1.png?width=1648&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e50099bce90cb39d5dda0a3890b46d914b11be9c", "score": 32}, {"text": "[P] Let's build ChatGPT. Hi all, I just made a tutorial on how to build a basic RLHF system on top of Andrej Karpathy's nanoGPT. I'm grateful to have gotten a thumbs up on Twitter from the legend himself, always a bit nerve wracking making this sort of thing.\n\nI'm sharing this here because I'd love to go deeper into teaching and building this out, if people are interested in watching this sort of thing. Would be very helpful to hear your thoughts.\n\nHere's the code:\n\nhttps://github.com/sanjeevanahilan/nanoChatGPT\n\nThe video: \n\nhttps://m.youtube.com/watch?v=soqTT0o1ZKo&amp;feature=youtu.be", "score": 362}, {"text": "[R] What do we think about Meta-Interpretive Learning?. Came across this concept, Meta-Interpretive Learning (MIL) developed by Muggleton, Patsantzis, et al.\n\n* [https://arxiv.org/pdf/2101.05050.pdf](https://arxiv.org/pdf/2101.05050.pdf)\n* [https://arxiv.org/pdf/2106.07464.pdf](https://arxiv.org/pdf/2106.07464.pdf)\n* [Presentation](https://www.youtube.com/watch?v=73cBWmjlFLk)\n\nFrom what I understand this is a relatively new approach to ML? Has anyone heard of this? I was hoping to get a general feel for what people in the industry believe for the perspectives of this approach. If you're curious, here's an [implementation](https://github.com/stassa/louise) of MIL.", "score": 3}, {"text": "[P] Orphic - A natural language interface for *nix systems (Powered by GPT). ", "score": 11}, {"text": "[D] Modern Topic Modeling/Discovery.  I was wondering what are the modern techniques for topic discovery for short and long text. It seems this topic to be slower advancing compared to the rest. I am aware of bertopic but tbh I always have issues finetuning it.\n\nOn a second thought I was thinking to use qna/chat gpt models in order to generate models, so I wanted to ask your opinion on some potential prompts that I could use. Essentially a bit of brainstorming. I will open source all the gathered ideas along with mine and share the link here :)", "score": 5}, {"text": "[P] We gave GPT-3.5 tools that developers use and let it use them in a sandboxed cloud environment (Demo). ", "score": 37}, {"text": "[R] [P] Statically parameterized gated MLPs help TabTransformer-like architecture to achieve state-of-the-art benchmarks for deep learning-based tabular modeling.. ", "score": 34}, {"text": "[R] Quantitative comparison of ChatGPT and GPT-4 performance on multiple open source datasets. Preliminary results give credence to some of the claims made by OpenAI regarding performance gains achieved by GPT-4 across domains. Unanswered questions remain regarding training data used and possible leakage. Tools used were Langchain and the current API endpoints (chatgpt-3.5-turbo and gpt-4).\n\nhttps://twitter.com/K_Hebenstreit/status/1636789765189308416", "score": 10}, {"text": "[P] I built a salient feature extraction model to collect image data straight out of your hands.. ", "score": 770}, {"text": "[D] Bayesian Optimization in ML. I am currently thinking of a possible topic for my research and I want to know what are the things that I need to know or study about Bayesian Optimization? Any idea is much appreciated. Thank you\u2026", "score": 3}, {"text": "[D] \"Glaze\" claims to be able to apply an invisible filter to images to prevent them being useful for training image models. Real tech, or a grift?. This tool \"Glaze\" has been picking up a lot of traction on social media from the anti-AI-image-generator camp, with the general claim being that any image can be \"protected\" from making a useful contribution if included in model training corpora by applying imperceptible filtering.\n\nWithout commenting on the arguments for or against whether this would be a good thing to exist, I am interested in hearing whether or not this capability actually exists, or whether people are once again leaning in hard for a false claim about a system they don't understand. I don't want to go digging through any technical information they've released because the whole conversation makes me want to tear my hair out and is rife with misinfo, but, from what I've actually heard on the technical side, it has sounded more like some sort of adversarial attack dependent on the latent space implementation of Stable Diffusion specifically, which would not \"protect\" their users from inclusion in other models. Even if the approach were to be extensible to incorporate adversarials against other models on the fly, this wouldn't retroactively protect already processed images from being used by future models with different internals. (I guess unless there was some sort of live system built into web image hosts, but we are not there yet...)\n\nAnyway, my gut instinct is that people are being mislead here based on their fear of their images being incorporated into training sets and \"stolen\", but before I make any strong claims to that effect it'd be nice to hear from someone who has more knowledge of the area.", "score": 38}, {"text": "[D] Do you find jobs that ask for knowledge/skills in multiple areas of AI unreasonable?. I have come across job posts that require candidates to have knowledge of machine learning, natural language processing and computer vision.\n\nWhenever I see this, I take it as a red flag. Each of these fields are pretty vast on their own. I don't find it reasonable for a company to expect the candidate to know them all.\n\nAlthough, I do realize these are posted by the HR team who usually don't have much idea about the roles. But still, I would at least expect someone from the department for which the position is for would at least skim through and validate the requirements. I mean, this guy is going to work with you, at least see what is being asked from them is accurate.\n\nAm I wrong here?", "score": 2}, {"text": "[R] ChatGLM-6B - an open source 6.2 billion parameter Eng/Chinese bilingual LLM trained on 1T tokens, supplemented by supervised fine-tuning, feedback bootstrap, and RLHF. Runs on consumer grade GPUs. ", "score": 197}, {"text": "[D] Totally Open Alternatives to ChatGPT. I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt\n\nBy alternative, I mean projects feature different language model for chat system.\nI do **not** count alternative **frontend** projects because they just call the API from OpenAI. \nI do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.\n\nTags:\n\n-   B: bare (no data, no model's weight, no chat system)\n-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)\n\n| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |\n| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |\n| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |\n| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |\n| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |\n| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local &amp; remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save &amp; Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |\n| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |", "score": 689}, {"text": "[P] [D] datasetGPT - A command-line tool to generate datasets by inferencing LLMs. Supports OpenAI, Cohere, and Petals.. ", "score": 74}, {"text": "[P] Semantic Feature Embeddings from Hashtags. I want to get semantic feature embeddings given a list of hashtags, to find similar users in social media data (using cosine similarity) or even do zero shot classification. I thought of using a BERT-like pretrained encoder language model, but I guess this is not optimal because grammar and word order do not matter in this case.\n\nDo you know such pretrained embedding model or have any tips, how to train such a model in an unsupervised way( I already have millions of posts containing hashtags)?", "score": 2}, {"text": "[P] I made a command-line tool to record dialogues between two ChatGPT agents or inference multiple LLM backends at scale.. ", "score": 8}, {"text": "[Research] Alpaca 7B language model running on my Pixel 7. &amp;#x200B;\n\nhttps://preview.redd.it/n9ctmf71xioa1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cefc80a00f5b0c2c27642154d27094e7ab2172e", "score": 153}, {"text": "Alpaca 7B Training - $75/Hour --&gt; Bay Area? [P]. Hi,\n\nI'm interested in using Alpaca 7B on my own computer and training it on my own datasets, however I know nothing about AI or machine learning. So, I'm looking for someone to teach me how to run and train Alpaca 7B on my laptop. I have specific texts and information I want to train it on and am hoping to find someone to show me how to do that. I'm not a programmer, but am very interested in the long term potential of AI. If you've already messed around with Alpaca and can help a neophyte, I would very much appreciate it! I think I would learn better in person, so if you're in the Bay Area that would be great!", "score": 0}, {"text": "[D] What is the best approach to create embeddings for time series with additional historical events to use with Transformers model?. The problem is: \n\n\\- We have multiple historical time series (e.g. air quality / temperature sensor measurements in different locations of the city).   \n\\- There is discreet information about events at certain time points which may correlate with measurements. E.g. crowded events, working hours, public holidays etc. \n\nI am trying to use transformers models to predict measurement values. The problem is how to feed all the data into transformer. \n\nIt seems like not a huge problem for time series values (I can try time2vec algo), but the question is how to embed information about discreet historical events?", "score": 6}, {"text": "[P] ML Feedback Widget: Use customer feedback as ground truth for your production ML models. ", "score": 0}, {"text": "[D] Question about multi-Head-Attention - more precisely about processing embedding dimensions.   \n\nSo I found two contradictory explanations of the MHA (multi-head-self-attention-module):\n\nIn **the first approach**, the input embedding (= the  input matrix) is split along the embedding dimension and all heads are  given a subset of the dimensions/features of each word. Some websites supporting this theory: [https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553](https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553)   \n\\-&gt; Quote: \"The input has been split into multiple heads, and we  are running the attention model separately on each of these heads.\"\n\n[https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3)   \n\\-&gt;  Quote: \"In multi-head attention we split the embedding vector into N  heads, so they will then have the dimensions batch\\_size \\* N \\* seq\\_len \\*  (d\\_model / N).\"\n\n**The second approach** assumes that all heads receive  the entire input data, but different weight matrices are used for each  head depending on the number of heads. This theory is well explained on [https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/](https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/)   \n\\-&gt; Quote: \"Each head is responsible to fully calculate the  attention for the whole embedding, not just for a subset of it and  creates h attention matrices\"\n\nI tend to the second explanation, but have not been able to find a satisfactory and contradiction-free answer so far.", "score": 11}, {"text": "[R] FateZero: Fusing Attentions for Zero-shot Text-based Video Editing. ", "score": 10}, {"text": "[D] Current best Voice cloning software?. I've been trying out Tortoise-tts to generate speech from custom voice samples, but it doesn't function that well with replicating irregular/dramatic voices. Are there currently any voice cloners that can give decent sounding speech from custom samples? And if you're more familiar with Tortoise, is there any adjustments I could make to make it sound better?", "score": 15}, {"text": "[D] Unit and Integration Testing for ML Pipelines. In the context of ML Pipelines (ETL, model training, model deployment and model serving scripts), are there any best practices on what test coverage to implement on these code artifacts?", "score": 25}, {"text": "[P] Web Stable Diffusion. Most of the existing stable diffusion demos rely on a server behind to run the image generation. It means you need to host your own GPU server to support these workloads. It is hard to have the demo run purely on web browser, because stable diffusion usually has heavy computation and memory consumption. The web stable diffusion directly puts stable diffusion model in your browser, and it runs directly through client GPU on users\u2019 laptop. \n\nThis means there is no queueing time for the server\u2019s response, more opportunities for client server co-optimizations, and friendly for personalization and privacy.\n\n&amp;#x200B;\n\nGithub page: [https://github.com/mlc-ai/web-stable-diffusion](https://github.com/mlc-ai/web-stable-diffusion)\n\nAlso comes with a online demo: [https://mlc.ai/web-stable-diffusion/](https://mlc.ai/web-stable-diffusion/)", "score": 103}, {"text": "[N] RadioGPT: 'World\u2019s first' AI-driven radio station is here. ", "score": 0}, {"text": "[D] ACL 2023 Conference Review Scores vs Acceptance. Let's investigate the initial review scores, review scores after rebuttal, and the final decision. Maybe future works can use this thread to study any correlation or trend?! \n\n:)\n\n[ACL 2023 Acceptance Recommendations](https://acl-org.github.io/blog/overall-recommendation/) has been released. According to Anna Rogers, it\u2019s based on single review and not the average score.", "score": 7}, {"text": "[D] Language model output based on only fixed set of value or variable either via prompting or fine-tuning. Since LLMs have capabilities to generate output in varieties of the form. I am looking a way where output is constrained based on fixed set of value. For example if I want to solve a mathematical equation or text to code generation, then typically LLMs generate unconstrained output based on its own knowledge. But what I am looking for is where output is constrained by limited set of variable or function name. I assume that to use these limited variable there need some intermediate steps which connects the limited variable to the text via manipulation of variable with intermediate function.  Like chain of thought, but in chain of thought variables or output are not constraints.", "score": 1}, {"text": "[R] Vid2Seq: a pretrained visual language model for describing multi-event videos. ", "score": 27}, {"text": "[D] LLama model 65B - pay per prompt. Hi,\n\nIs there any way to run llama (or any other) model in such a way, that you only pay per API request?\n\nI wanted to test how the llama model would do in my specific usecase, but when I went to HF Interface Endpoints it says that I would have to pay over 3k USD per month (ofc I do not have that much money to spend on a side-project).\n\nI would like to test this model by paying on per request basis.", "score": 0}, {"text": "[D] PyTorch 2.0 Native Flash Attention 32k Context Window. Hi,\n\nI did a quick experiment with Pytorch 2.0 Native scaled\\_dot\\_product\\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6csxe28lv9oa1.png?width=607&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1db074eaea9bb6d0b95678c2cfe39dc71cb48adf\n\nI think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention &amp; fine-tune on 32k tokens.\n\n**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \\~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.\n\n**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/o2hb25w1sboa1.png?width=1226&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1c7c1eda0e20f5123ea7c143a286aa9bb9a48491\n\n**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:\n\n[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)\n\nI will post an update after the weekend once the training has progressed somewhat.\n\n**Post-Weekend Update**: After \\~50k iterations (the model has seen \\~200 million tokens, I know this is just too small compared to 10s of billions trained by giga corps), loss only dropped from 4.6 to 4.2 on The Pile:\n\nhttps://preview.redd.it/vi0fpskhsuoa1.png?width=1210&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3fab4c767ac0cc8b7598d20566a77476e75efea1\n\nAFAIR, the loss of GPT-2 on the Pile if trained with 1024 tokens is \\~2.8. It seems like the size of the dimension for each token is kind of limiting how much loss can go down since GPT-2 (small) has an embedding dimension of 768. Maybe someone can experiment with GPT-2 medium etc. to see how much we can improve. This is confirmation of the comment by u/lucidraisin [below](https://www.reddit.com/r/MachineLearning/comments/11tmpc5/comment/jcl2rkh/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3).", "score": 335}, {"text": "[D] An Instruct Version Of GPT-J Using Stanford Alpaca's Dataset. I  just released an instruct version of GPT-J using Stanford Alpaca's  dataset.The result of this experiment is very cool and confirms that,  when fine-tuned on the right data, GPT-J is a very powerful AI model!You  can download the model from the HuggingFace hub: [https://huggingface.co/nlpcloud/instruct-gpt-j-fp16](https://huggingface.co/nlpcloud/instruct-gpt-j-fp16)\n\nHere is an example:\n\n`from transformers import pipeline import torch`\n\n`generator = pipeline(model=\"nlpcloud/instruct-gpt-j-fp16\", torch_dtype=torch.float16, device=0)`\n\n`prompt = \"Correct spelling and grammar from the following text.\\nI do not wan to go\\n\" print(generator(prompt))`\n\nMore details about this experiment here: [https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html](https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html?utm_source=reddit&amp;utm_campaign=nwu8d596-3816-11ed-a261-0242ac140007)\n\nI hope it will be useful! Please don't hesitate to share some feedbacks!\n\nJulien", "score": 142}, {"text": "[R] ViperGPT: Visual Inference via Python Execution for Reasoning. [https://viper.cs.columbia.edu/](https://viper.cs.columbia.edu/)\n\nPaper - [https://arxiv.org/abs/2303.08128](https://arxiv.org/abs/2303.08128)", "score": 49}, {"text": "[R] Online AI Game Announcement. Hi all!     \n\n\nI\u2019m a PhD student at Stanford working on foundation models, and thought one of my recent research projects would be of interest to this community.   \n\n\nWe just released on online game (link in comments) where you collaborate with a text-to-image AI model to create target images and compete with players around the world. Takes only 3 min to play (to get a high score you may need to play longer) and helps us study Human-AI collaboration. New image challenges are released daily. Try out the game and share with your friends!", "score": 52}, {"text": "[R] The Philosophy of Deep Learning - free conference next weekend with live streaming. ", "score": 1}, {"text": "[N] Jumpy 1.0 has now been released by the Farama Foundation. Jumpy 1.0 is now live, and the project is stable and mature.\n\nJumpy is a lightweight project for easily switching between Jax and Numpy functions that can serve as a drop-in replacement for Jax. This allows for writing one codebase that can use either backend, allowing for creating codebases that work with either data structure type or easier debugging of code. This project is already being used in Gymnasium to create environment wrappers that can support both Numpy and Jax-based hardware accelerated environments. We plan to continue improving the project with support for PyTorch functions, all Numpy functions and more functionality to support enabling or disabling different backends\n\nYou can read the full release notes here: [https://github.com/Farama-Foundation/Jumpy/releases/tag/1.0.0](https://github.com/Farama-Foundation/Jumpy/releases/tag/1.0.0)", "score": 20}, {"text": "LLMs are getting much cheaper \u2014 business impact? [D]. Saw this out of Stanford. Apologies if it\u2019s been shared here already. \n\n*We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI\u2019s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (&lt;600$).*\n\nBasically, starting w an open source Meta 7B LLaMa model, they recruited GPT-3.5 to use for self-instruct training (as opposed to RLHF) and were able to produce a model that behaved similar to GPT-3.5. Amazingly, the process only took few weeks and $600 in compute cost.  \n\nAny thoughts on how such low cost to train/deploy LLMs could affect companies like AMD, Nvidia and Intel etc? This seems like new idiom of AI tech and trying to wrap my head around CPU/GPU demand implications given the apparent orders of magnitude training cost reduction. \n\nLink: https://crfm.stanford.edu/2023/03/13/alpaca.html", "score": 289}, {"text": "[D] ACL 2023 paper reviews.. The reviews for ACL 2023 papers are expected to be released soon, and this post aims to start a conversation about the same. Let's share our thoughts and feelings about the joys and pains of paper reviews!", "score": 42}, {"text": "[D] Are there any open source feature stores that do not rely on K8s?. We have investigated some open source feature stores like Feast and FeatureForm, but most require Kubernetes to deploy on the cloud. Unfortunately, our organization isn't very mature in adopting Kubernetes. Are there any recommended feature stores that don't require K8s to deploy its infrastructure?", "score": 0}, {"text": "[R] RWKV 14B ctx8192 is a zero-shot instruction-follower without finetuning, 23 token/s on 3090 after latest optimization (16G VRAM is enough, and you can stream layers to save more VRAM). I try the \"Alpaca prompt\" on RWKV 14B ctx8192, and to my surprise it works out of box without any finetuning (RWKV is a 100% RNN trained on 100% Pile v1 and nothing else):\n\nhttps://preview.redd.it/fciatottq7oa1.png?width=1046&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=891904adbadefb5902b86f67098c852da88dc167\n\nYou are welcome to try it in RWKV 14B Gradio (click examples below the panel):\n\n[https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio](https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio)\n\nTips: try \"Expert Response\" or \"Expert Long Response\" or \"Expert Full Response\" too.\n\nhttps://preview.redd.it/qo71b85vq7oa1.png?width=2516&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4b1717754d03e28b4bba01530672935407e7797\n\n===================\n\nChatRWKV v2 is now using a CUDA kernel to optimize INT8 inference (23 token/s on 3090): [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nUpgrade to latest code and \"pip install rwkv --upgrade\" to 0.5.0, and set os.environ\\[\"RWKV\\_CUDA\\_ON\"\\] = '1' in v2/chat.py to enjoy the speed.\n\nThe inference speed (and VRAM consumption) of RWKV is independent of ctxlen, because it's an RNN (note: currently the preprocessing of a long prompt takes more VRAM but that can be optimized because we can process in chunks).\n\nMeanwhile I find the latest RWKV-4-Pile-14B-20230313-ctx8192-test1050 model can utilize a long ctx:\n\nhttps://preview.redd.it/a68dw0hzq7oa1.png?width=398&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=307e4d7847cb03cab3930b3ea07e9b2f856c9b1c", "score": 103}, {"text": "[R] Memory Augmented Large Language Models are Computationally Universal. ", "score": 199}, {"text": "[D] instruction tuning : what should I read?. I think I have a decent grasp on transformers, LLMs, prompting, one/few shot learning, fine-tuning. But till now I haven't studied instruction fine tuning and the technique has outgrown my expectations. \nWhere should I start reading about it?\nDo you know any good literature review article to suggest ?", "score": 6}, {"text": "[P] nanoT5 - Inspired by Jonas Geiping's Cramming and Andrej Karpathy's nanoGPT, we fill the gap of a repository for pre-training T5-style \"LLMs\" under a limited budget in PyTorch. We release the code to reproduce the pre-training of a \"Large Language Model\" (T5) under a limited budget (1xA100 GPU, \\~20 hours) in PyTorch. We start from the randomly initialised T5-base-v1.1 (248M parameters) model implemented in HuggingFace. Next, we pre-train it on the English subset of the C4 dataset and then fine-tune it on Super-Natural Instructions (SNI).\n\n**In \\~20 hours on a single GPU, we achieve \\~40 RougeL on the SNI test set, compared to \\~42 RougeL of the original model available on HuggingFace Hub and pre-trained through \"a combination of model and data parallelism \\[...\\] on slices of Cloud TPU Pods\", each with 1024 TPUs.**\n\nOur core contribution is not the T5 model itself, which follows the HuggingFace implementation. Instead, we optimise everything else in the training pipeline to offer you a user-friendly starting template for your NLP application/research.\n\nWe are keen to hear your suggestions to improve the codebase further.\n\n&amp;#x200B;\n\nGithub: [https://github.com/PiotrNawrot/nanoT5](https://github.com/PiotrNawrot/nanoT5)\n\nTwitter: [https://twitter.com/p\\_nawrot/status/1636373725397520384](https://twitter.com/p_nawrot/status/1636373725397520384)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/zluas7u235oa1.png?width=1152&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8d642abdce1753841b7fc977a141d0f13ca2b213", "score": 258}, {"text": "[D] Generate diverse candidates with T5?. Hey guys, I am trying to generate masked span predictions with T5 to use for teacher student distillation. Because of this method, I need the teacher generate a diverse set of predictions, so the student can be trained to match its distribution. However, even when changing parameters like temperature to obscene values (1000+), the teacher still generates the same things every time, and the temperature value doesn\u2019t seem to affect the generation at all. I have also tried beam search, top p, and others. Any ideas how I can do this?", "score": 3}, {"text": "[D] Our community must get serious about opposing OpenAI. OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.\n\nThey have abandoned this idea entirely.\n\nToday, with the release of GPT4 and their direct statement that they will not release details of the model creation due to \"safety concerns\" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.\n\nAI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.\n\nI get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.\n\nWe need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.\n\nThis conversation will only ever get more important.", "score": 2554}, {"text": "training KGE model [Project]. I have knowledge graph : 24 relationships, 11 entities , &gt; 20K facts   (rows). What I need is the embedding for only one entity, out of those 11. Once the training is completed, I will extract those embeddings and  use them to train a separate GNN model.  \nMy idea was to over-fit  the  KGE model and use all data for training. Given my use case I don't  see  why a test set is needed. Once the model is trained, I will  evaluate it  on the train set, if MRR/ Hits@10 are good, I would extract  the embedding  and mode forward. If not, I will test a different model  and iterate.  \nAm I doing something stupid ?", "score": 7}, {"text": "[N] A $250k contest to read ancient Roman papyrus scrolls with ML. Today we launched [the Vesuvius Challenge](https://scrollprize.org/), an open competition to read a set of charred papyrus scrolls that were buried by the eruption of Mount Vesuvius 2000 years ago. The scrolls can't be physically opened, but we have released 3d tomographic x-ray scans of two of them at 8\u00b5m resolution.  The scans were made at a particle accelerator. \n\nA team at UKY led by Prof Brent Seales has [very recently demonstrated](https://scrollprize.org/tutorial4) the ability to detect ink inside the CT scans using CNNs, and so we believe that it is possible for the first time in history to read what's in these scrolls without opening them. There are hundreds of carbonized scrolls that we could read once the technique works \u2013 enough to more than double our total corpus of literature from antiquity.\n\nMany of us are fans of /r/MachineLearning and we thought this group would be interested in hearing about it!", "score": 275}, {"text": "[D] Creating an open platform for collecting corrective feedback on conversational ML products &amp; projects. Any applied scientist or engineer working with deep learning would tell you that corrective info/feedback is the only way up (especially for massive deep networks). With some of my fellows, we are watching what has been happening in the last couple of months with quite a shock and wonder as the community is throwing valuable feedback (for free) to a (closed source) company from all available online channels (e.g., Reddit, Twitter, Github, blogs), boosting their models (again for free).\n\nI should better note here that this is what they need to go from GPT-4 to v5, or GPTX (folks love X these days).\n\nThere are also valuable calls to action from the community. As a start, community can attempt to create an open initiative to organize all the feedback thrown to open or closed (e.g., GPT-4) conversational models/papers/products. One may argue this will make companies' work even easier, but if there is a resource to mine, it will be mined. Therefore creating a (legal) initiative may work in the favor of the community.\n\nWe should consider that conversational DL (maybe in the far future AI, not sure about that yet) becoming like the commercial aircraft industry where the only way to succeed is to fall and enforce what went wrong. Although the community is very excited now, folks may (probably will) get saturated, and the new companies and initiatives in the future may not get the same amount of feedback. \n\nThis may create a monopoly, so it can be a better idea now to discuss the options how we can unify these valuable resources.", "score": 12}, {"text": "[N] bloomz.cpp: Run any BLOOM-like model in pure C++. [bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) allows running inference of BLOOM-like models in pure C/C++ (inspired by llama.cpp). It supports all models that can be loaded with `BloomForCausalLM.from_pretrained()`. For example, you can achieve 16 tokens per second on a M1 Pro.", "score": 21}, {"text": "[N] PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever. Preview of the post since it's dropping in a few hours: https://deploy-preview-1313--pytorch-dot-org-preview.netlify.app/blog/pytorch-2.0-release/\n\n\nAlso a post about Accelerated Diffusers with 2.0: https://deploy-preview-1315--pytorch-dot-org-preview.netlify.app/blog/accelerated-diffusers-pt-20/\n\nGPT Summary:\n\n- PyTorch 2.0 is a next generation release that offers faster performance and support for dynamic shapes and distributed training using torch.compile as the main API.\n\n- PyTorch 2.0 also includes a stable version of Accelerated Transformers, which use custom kernels for scaled dot product attention and are integrated with torch.compile.\n\n- Other beta features include PyTorch MPS Backend for GPU-accelerated training on Mac platforms, functorch APIs in the torch.func module, and AWS Graviton3 optimization for CPU inference.\n\n- The release also includes prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.", "score": 211}, {"text": "[D] Comparison of the Model prediction uncertainty of two different models. In your career as data scientists have you ever faced the situation where you have to compare the quality of the predictive uncertainty estimation of a machine learning model with an old statistical model that was already in use? if so, how did you do it?\n\ni have a bnn trained on some experimental data and a statistical models developed by my department that depends on some parameters estimated through the classic mcmc methods. Both seems to agree well with the experimental data but i wanted to compare the quality of the model predictive uncertainty\n\n&amp;#x200B;\n\ni thought about comparing the level of calibration of the uncertainty but  i am not sure if i have to do it on the test dataset (due to the bnn) or the entire dataset ( due to the fact that for the old statistical model they use mcmc methods on the entire dataset)", "score": 4}, {"text": "[D] paperspace or another cloud service. \ni'm working on colab, and i have 25GB of ram but that's not enough for my case and my notebook always end up crashing (only reading data, i haven't even started the training process).\n\ni considering passing to paperspace gradient but can i access my data stored in drive with it ? ( in colab it's easy to mount it),  and if i move the data from drive to github repo can i access it then ?", "score": 6}, {"text": "In your experience, are AI Ethics teams valuable/effective? [D]. Hello! I read the following article about Microsoft laying off their AI Ethics team: https://www.cmswire.com/customer-experience/microsoft-cuts-ai-ethics-and-society-team-as-part-of-layoffs/\n\nIn your experience, what value do AI ethics teams add? Do they actually add useful insight, or do they serve more as a PR thing? I\u2019ve heard conflicting anecdotes for each side. Is there anything you think AI ethics as a field can do to be more useful and to get more change? Thanks!", "score": 46}, {"text": "[D] Anyone else witnessing a panic inside NLP orgs of big tech companies?. I'm in a big tech company working along side a science team for a product you've all probably used. We have these year long initiatives to productionalize \"state of the art NLP models\" that are now completely obsolete in the face of GPT-4. I think at first the science orgs were quiet/in denial. But now it's very obvious we are basically working on worthless technology. And by \"we\", I mean a large organization with scores of teams. \n\nAnyone else seeing this? What is the long term effect on science careers that get disrupted like this? Whats even more odd is the ego's of some of these science people\n\nClearly the model is not a catch all, but still", "score": 1288}, {"text": "[N] Mozilla launched a responsible AI challenge and I'm stoked about it. who's applying and what are you planning to build???  [https://www.axios.com/2023/03/15/mozilla-responsible-ai-challenge](https://www.axios.com/2023/03/15/mozilla-responsible-ai-challenge)", "score": 36}, {"text": "[D] Any other ICML reviewers noticing strange scores for the papers they're assigned to?. I'm reviewing 4 papers, of which I gave one a very positive review. I am the only negative reviewer for 3/4 of the papers I am reviewing. Most of the papers have short, glowing positive reviews that don't meaningfully engage with the paper at all. At least two of the papers have bizarre formatting problems like blurry figures with unreadable text (not publication quality) that don't pass the eye test.\n\nA similar thing happened at ICLR reviews this year, and the authors withdrew their papers in spite of having 2x very positive reviews and 1x slightly negative review (mine). No attempt at rebuttal.\n\nHas anybody else experienced this?", "score": 22}, {"text": "[D] GPT-3 will ignore tools when it disagrees with them. [https://vgel.me/posts/tools-not-needed/](https://vgel.me/posts/tools-not-needed/)", "score": 26}, {"text": "[D] To those of you who quit machine learning, what do you do now?. I'm currently doing my master's degree and have been set on a DL-related career for a while. But recently I noticed it doesn't bring me joy.\n\nComing up with architectures that randomly work/don't work, tuning parameters, waiting for days till the model is trained... the level of uncertainty is just too high for me. Because of that, I don't feel productive working on it and I'm slowly considering switching to another IT field.\n\nFor those of you who quit machine learning (especially deep learning):\n\n1. What did you switch to?\n2. Are you satisfied with your new job? (Is it stressful/intellectually challenging? Is it possible to keep it 9-5?)\n3. How to ensure a smooth transition to that field?\n\nThanks in advance!\n\n\\_\\_\\_  \nPS I know machine learning isn't all about deep learning, but in my current subfield (computer vision), mostly deep learning is used.", "score": 56}, {"text": "[D] 5 days ago I asked a question.. First of all, Id like to apologize for my last post, I could have definitely worded it better. \n\nNow then:\n\nWith the release of GPT4, being multimodal, 32k tokens, and being a pretty decent upgrade from ChatGPT in general, it does seem like some ML fields will be completely dominated by these MLLMs.\n\nSaw some PhD and PhD candidates freaking out too.\n\nSo it begs the question: are you folks still confident that there is room for independent jobs in NLP and computer vision?\n\nOnce again, would love to hear your answers.", "score": 0}, {"text": "[D] Is there an expectation that epochs/learning rates should be kept the same between benchmark experiments?. I've found that by dramatically lowering the LR and increasing the number of epochs, very simple, baseline models can outperform SoTA models which use far more parameters. Is this considered \"cheating\" when comparing models? Is this something interesting enough to warrant a short paper? I'm not sure what to do with this information. \n\nFor example, in the original [VGAE](https://arxiv.org/pdf/1611.07308v1.pdf) paper, when training a GAE, they use a LR of 0.01, and train for 200 epochs to get 0.91 AUC, 0.92 AP on a link prediction experiment. Rerunning the same experiment with a LR of 5e-5 for 1500 epochs gets 0.97 AUC, 0.97 AP which is better than the current leader on papers with code for this dataset. \n\nIt needs more epochs but has way, way fewer parameters than SoTA models, is this a valid trade-off? Is this even a fair comparison?", "score": 18}, {"text": "[P] 24 Fugues (music) in the style of J.S. Bach. Completely generated by a BERT inspired transformer model.. [Here](https://soundcloud.com/loua19/sets/ai-bach-fugues) are the samples. My favourite is [this](https://soundcloud.com/loua19/fugue-10?in=loua19/sets/ai-bach-fugues&amp;si=c3d599e6eee24766b92c9d619a464826&amp;utm_source=clipboard&amp;utm_medium=text&amp;utm_campaign=social_sharing) one! Which one is your favourite?\n\nThese samples are the product of a transformer (encoder) model trained on only 3 hours of music. Each sample is seeded by the first four bars of a real piece of music. These are the final samples before I completely overhaul the pre-training stage. The idea is to go from about 2-hours of midi to over 500 hours. I'm very excited to see how this effects the sample quality.\n\nIf anyone in interesting in following the project. Star the [GitHub](https://github.com/loua19/counterpoint) and follow me on [Twitter](https://twitter.com/loua42).", "score": 24}, {"text": "[D] GPT-4 Speculation. Hi,\n\nSince GPT-4 paper does not contain any information about architectures/parameters, as a research or ML practitioner, I want to speculate on what they did to increase the context window to 32k.\n\nBecause for the type of work I do, a 4k or 8k token limit is pretty much useless. I have seen open-source efforts focused more on matching the number of parameters and quality to the closed-source ones but completely ignoring a giant elephant in the room, i.e., the context window. No OSS model has a context window greater than 2k tokens.\n\nI would love to hear more thoughts on the model size (my guess is \\~50 B) and how they fit 32k tokens in 8xH100 (640 GB total) GPUs.", "score": 71}, {"text": "[D] Alternatives to Mediapipe's FaceMesh for 3D Face Reconstruction. Hi there,\n\nCurrently, I am using mediapipe for FaceMesh, which has decent reliability and is easy to setup in Python. However, I recently discovered Microsoft Research's \"3D Face Reconstruction with Dense Landmarks\" paper, which appears to be a much better alternative.\n\nDoes anyone know where I can access Microsoft DenseLandmarks or an equally good alternative?", "score": 13}, {"text": "[News] OpenAI Announced GPT-4. Research blog:\n\n[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)\n\nProduct demo:\n\n[https://openai.com/product/gpt-4](https://openai.com/product/gpt-4)\n\nResearch report:\n\n[https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf)\n\nAPI waitlist:\n\n[https://openai.com/waitlist/gpt-4-api](https://openai.com/waitlist/gpt-4-api)\n\nTwitter announcement:\n\n [https://twitter.com/OpenAI/status/1635687373060317185](https://twitter.com/OpenAI/status/1635687373060317185)\n\nOpenAI developer livestream:\n\n[https://www.youtube.com/watch?v=outcGtbnMuQ](https://www.youtube.com/watch?v=outcGtbnMuQ&amp;ab_channel=OpenAI)", "score": 701}, {"text": "[R] ConvNextV2. Hello,\n\n\nI was reading the Convnext2 paper. Apparently they added what they call a global normalization layer to encourage features diversity. I understand the equations but I fail to understand how it encourages features diversity. If anyone have any clue I will grateful.\n\n\nThanks !", "score": 8}, {"text": "[D] Vegetarian Wolves and Stochastic Parrots: The Future of Prompt Engineering with GPT-4?. In today's announcement on Hacker News I saw an incredulous comment pointing out GPT-4's failure to solve variations of the wolf, goat, and cabbage problem, using this to dismiss it as anything more than a stochastic parrot.\n\nBut in my own experience with GPT-4 though Bing chat, I'm constantly being reminded of Li et al *Emergent World Representations: Exploring a\rSequence Model Trained on a Synthetic Task* (2023).\n\nSo I tried a variation of this puzzle with a vegetarian wolf and a meat-eating goat.\n\nIt absolutely did mess up generating an answer, but it also appeared to be able to identify where it was making mistakes under Socratic follow up questioning. It just couldn't get the solution out, and I knew there was a way to help engineer it out of this rut if only I could break the predictive aspects of the text which appeared to be masking a deeper semantic understanding of the problem.\n\nSo I asked it. In a fresh chat I described what was happening with predictive text and asked if it could write a prompt that avoided this issue, and the rather clever version it generated replaced the problematic nouns with emoji representations.\n\nThis trick worked brilliantly combined with a slight chain of thought prompt (per Wei et al) and enforcing repetition of variant adjectives to avoid falling back into the classic solution.\n\nBut not only did this work for the initial prompt, when I'd give it the word-only version and it would get tripped up, asking it to convert nouns to emojis while it worked through the logic and only converting back to words at the end was a *significantly* better outcome while challenging its responses than asking it to rethink erroneous steps only in words.\n\nThe idea that GPT-4 broadly fails at variations of this problem is a false negative. Yes, its nature is a LLM and as such it **is** prone to getting tripped up on natural language output too similar to common sequences in training data.\n\nBut symbolic representation as a replacement can untrip it, and I suspect from here on out with LLM models we will see prompt engineering moving further from just providing local contexts to trigger intended frequency associations and towards engaging abstractions to *avoid* frequency associations and trigger whatever world representations might have been established during training more directly.\n\nFor anyone who would like to try this out for themselves, here's the prompt that gets Bing chat in Creative mode (and likely GPT-4 directly) to solve the aforementioned puzzle correctly multiple times in a row:\n\n&gt; Without searching, solve the following puzzle making sure to repeat any adjectives describing an emoji each time you mention it: A man wants get to the other side of a river. With him he has a vegetarian \ud83d\udc3a, a \ud83d\udc10 that only eats meat, and a \ud83e\udd6c. The man has a boat that can only take him and one of the things he has with him to the other side. How can he do this without anything being eaten? (Think carefully, as this is specifically designed to be harder for you than it looks. In fact, before giving an answer, describe who would eat whom if left on the same side.)\n\n(For reference, ChatGPT gets the first part of the chain of thought correct in identifying who eats whom but immediately spits out an emoji version of the classic solution.)", "score": 24}, {"text": "[P] Multimedia GPT: Can ChatGPT/GPT-4 be used for vision / audio tasks just by prompt engineering?. The newly released GPT-4 allows users to upload images, but we're still far from having a truly capable multimodal model. So we built this project as a feasibility study (and for fun!) to see how much we can do with just tuning the prompts. In short, we try to \"connect\" different models (vision, audio, etc) via carefully designed prompts.\n\nMultimedia GPT connects your OpenAI GPT with vision and audio. You can now send images, videos (in development), and even audio recordings using your OpenAI API key. We base our project on Microsoft's Visual ChatGPT, which achieves some success just by tuning the prompts.\n\nCheck-out our project [here](https://github.com/fengyuli2002/multimedia-gpt)! We also have a cool demo where Multimedia GPT successfully understands a person telling a story!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6x6pjamt30oa1.png?width=3024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=30f6c9e5b9329642ebda40241f4ac2aca464c4d8\n\nhttps://preview.redd.it/3dr5tamt30oa1.png?width=2950&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9b3fc71822a7b1f9bc008ffb57b49b6b2c4bfb6d\n\nAny suggestion is appreciated\\~", "score": 0}, {"text": "[D]Will the AI use and distribution be under strict government control?. I mean, it is not too far fetched to imagine the governments will try to limit the use of AI for deepfakes etc., and mere possession of those AIs capable of those things, or distribution of tools capable of that, will end of the same spectrum as possession / distribution of child porn.\n\nI can easily see huge pushback for regulation once we get to stage where everyone can run AIs on their home computers with minimal setup and they will became so good at generating stuff it will not be distinguishable from the real thing.", "score": 0}, {"text": "[D] When to expect announcement of accepted workshops for IJCAI?. According to their schedule, IJCAI has sent acceptance notification to workshops organizers at March 6th. When should we expect that the accepted workshop list will be available?", "score": 6}, {"text": "[Discussion] What happened to r/NaturalLanguageProcessing ?. It seems to be locked right now. Was there brigading or sth of that fashion?", "score": 5}, {"text": "[D] Challenges for Keras as a Deep Learning Framework.  Hey, I've been using Keras for a while now and I think it's a great deep learning framework, but there are some challenges that prevent it from overtaking PyTorch. Here are the main ones:\n\nFirstly, Keras' customer support can be pretty inadequate. I've had issues with memory leaks and race conditions that were hard to reproduce, and the customer service team didn't investigate the problem or work with me to track it down. They also sometimes ignore tickets or requests for documentation fixes, which can be frustrating.\n\nAnother issue is that the functional programming interface in Keras has some limitations. While it's good for people who think in a functional way, the graph system in TensorFlow isn't generalized or abstracted well. This can create artificial boundaries in the graph processor for models of models, which isn't mathematically sound. Plus, accessing nodes in the graph isn't straightforward, which is a sign that there are underlying issues with the graph abstraction. These limitations need to be addressed to make the functional interface more robust.\n\nLastly, Keras has limited support for algebra beyond real numbers, like complex numbers. Metrics calls cast complex numbers to their real parts, which shows that Keras assumes only real-valued data is processed by the graphs. This approach is short-sighted and limiting for a framework that markets itself as comprehensive.\n\nDespite these challenges, Keras is still a popular choice for research code development because it's faster to develop than PyTorch in many cases. However, Keras needs to address these limitations to stay competitive in the research community. Improving customer support, expanding support for complex numbers, and addressing the limitations of the functional interface would create a more satisfied and productive user base.", "score": 0}, {"text": "[N] Baidu to Unveil Conversational AI ERNIE Bot on March 16 (Live). Baidu will unveil its conversational AI ERNIE Bot, powered by Baidu's in-house LLMs, on March 16. The ERNIE LLM was first proposed as a language understanding model in 2019 and evolved to ERNIE 3.0 Titan with 260 billion parameters.\n\nERNIE 1.0: [https://arxiv.org/abs/1904.09223](https://arxiv.org/abs/1904.09223)\n\nERNIE 2.0: [https://arxiv.org/abs/1907.12412](https://arxiv.org/abs/1907.12412)\n\nERNIE 3.0: [https://arxiv.org/abs/2112.12731](https://arxiv.org/abs/2112.12731)\n\nERNIE for text-to-image: [https://arxiv.org/abs/2210.15257](https://arxiv.org/abs/2210.15257)\n\nERNIE Bot live-stream on YouTube: [https://www.youtube.com/watch?v=ukvEUI3x0vI](https://www.youtube.com/watch?v=ukvEUI3x0vI)", "score": 31}, {"text": "[D] Choosing Cloud vs local hardware for training LLMs. What's best for a small research group?. We have a 20-40k budget at our lab and we are interested in training LLMs on data that is protected by HIPAA which puts restrictions on using just any cloud provider. We'd need a compute environment with 256gb vram.\n\nWould it be better to use AWS EC2 P3 instances or Google Cloud instead of trying to build our own server for this? We could spend the budget on a local server, but would this be obsolete within 2 years once the next gen GPUs are released?", "score": 11}, {"text": "[D] 2022 State of Competitive ML -- The Downfall of TensorFlow. It's shocking to see just how far TensorFlow has fallen. The 2022 state of competitive machine learning report came out recently and paints a very grim picture -- only 4% of winning projects are built with TensorFlow. This starkly contrasts with a few years ago, when TensorFlow owned the deep learning landscape. \n\nOverall, poor architectural decisions led to abandonment from the community, and a monopoly-style view of ML led to a further lack of adoption from necessary tool chains in the ML ecosystem. The TensorFlow team tried to fix all of this with the\u00a0TensorFlow v2 refactor, but it was too little, too late, and it abandoned the core piece TensorFlow was still holding on to \u2014 legacy systems.\n\nCheck out more here: [https://medium.com/@markurtz/2022-state-of-competitive-ml-the-downfall-of-tensorflow-e2577c499a4d](https://medium.com/@markurtz/2022-state-of-competitive-ml-the-downfall-of-tensorflow-e2577c499a4d)", "score": 251}, {"text": "[R] Is there any NeRF labelling standard?. I recently decided to introduce NeRFs in my research. So far, I got a theoretical understanding, but I am struggling to get an overview of the implementation side. I checked multiple implementations and realised there is no standard labelling format. Here, the labels are the camera parameters of each image required by NeRF. I have seen a tendency to use a .json file typically called \"transforms\". \n\n&amp;#x200B;\n\nI am mentioning this because I am creating a NeRF dataset of synthetic objects, and I found myself slightly lost when creating labels for the available implementations. I thought of getting some advice from you guys. \n\n&amp;#x200B;\n\nSo far, I have the multiview images and the camera parameters of each image in multiple txt files, each with the camera parameters of the respective image. My initial idea was to write a code to transform these .txt files into json files with the same format as the \"transforms\" files. Before, I would like to ask your opinion about the \"transforms\" format. Also, I would like to ask you if you are aware of an alternative method/code/repo to generate these NeRF labels.\n\n&amp;#x200B;\n\nThank you so much beforehand,", "score": 2}, {"text": "[D] Testing regimes for Data Science &amp; MLOps code. Hi folks. I'm looking into best practice for serving models in production environments (e.g. MLOps or equivalent buzz-term). We want to include testing for our models as part of a CI/CD pipeline, but are at a bit of a loss as to what this should look like. I've seen plenty of blog posts on using different packages to carry out testing, but remain unsure what the tests should look like / actually be testing for.\n\nFor example, say I have a model that takes in a text string, uses NER to identify key entities, and redacts a subset of those entities (in this case, for removal of Personally Identifiable Information) before returning the redacted text string - what would a robust unit test look like for such a model? Would it be suitable to just test that text in gives text out, or do we need to be testing that the expected entities are redacted, or are both tests required?\n\nAny tips on standard testing approaches for different problems would be appreciated, like regression problems, computer vision and NLP.", "score": 0}, {"text": "[D] ChatGPT Plus waitlist. I was surprised to find that ChatGPT plus (currently the only way to test a vanilla GPT-4 model) is not only behind a pay wall, it is also behind a \"wait wall\"!\n\nHas anyone played with GPT-4 yet? Is it as good as the paper suggests? Anyone got any idea how long the wait list is for access?", "score": 0}, {"text": "[Discussion] Huggingface for AI tooling. Hey all,\n\nI am having a difficult time keeping up with all the open-source tooling that is coming up lately. Huggingface is great for finding out about new models, data sets etc. but I am really curious if there is a community hub for AI tooling as well - for things like Langchain, LlamaIndex, Weaviate, Pynecone, Helicone etc.\n\nIdeally I would love to have a hosted option of those as well. To consume them easily like HF inference APIs.", "score": 6}, {"text": "Modern language models refute Chomsky\u2019s approach to language [R]. [https://lingbuzz.net/lingbuzz/007180](https://lingbuzz.net/lingbuzz/007180)", "score": 2}, {"text": "[D] On research directions being \"out of date\". For  the papers we have submitted in recent years, there has been a  significant increase in the number of reviewers whose only complaint is the paper not following a \"hip\" version of the research topic. They don't care about the results and don't care about the merit of the work,  their problem is that our work does not follow the trend. It feels like  there is this subset of reviewers see anything that is more than a year old as \"out of date\" and a reason for rejection.\n\nHave we been unlucky with our reviewer bingo recently or is this the case for others as well?", "score": 23}, {"text": "[P] Enriched Huggingface dataset (+embeddings, baseline, edge cases) for the DCASE Anomalous Sound Detection challenge. Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning),\n\nthe [DCASE sound event detection challenges](https://dcase.community/challenge2023/) have recently started!\n\nGenerally speaking, challenges are a big part of the ML community. These are typically very model-centric: The dataset is given in terms of datapoints/labels and the evaluation is purely quantitatively.\n\nIn real-world use cases, it is often a better idea to iterate on the data (data-centric AI, DCAI). We believe that this view can also be beneficial in a challenge setting. \n\nIn order to popularize this DCAI approach, we have built an enriched Huggingface dataset for the [DCASE Task2 Challenge](https://dcase.community/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring): [https://huggingface.co/datasets/renumics/dcase23-task2-enriched](https://huggingface.co/datasets/renumics/dcase23-task2-enriched)  \n\n\nhttps://preview.redd.it/wtv1b9ai7pna1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b9b0c9f735aa1f3b179b28b6756d37d28820ee07\n\nThe dataset can be loaded with a few lines of code and allows you to quickly:\n\n1. Understand the data distribution based on embeddings and manual inspection\n\n2. Understand critical data points based on baseline and anomaly detection results\n\n3. Leverage the HF model ecosystem for your trainings\n\n&amp;#x200B;\n\nWould love to hear honest feedback on this. If you find concrete problems in the workflow, feel free to submit an issue on our Github: [https://github.com/Renumics/spotlight](https://github.com/Renumics/spotlight)\n\nWe are currently thinking which benchmark datasets we should do next. Is there a dataset that you could recommend?\n\nBest,\n\nStefan", "score": 12}, {"text": "[R] Stanford-Alpaca 7B model (an instruction tuned version of LLaMA) performs as well as text-davinci-003. According to the authors, the model performs on par with text-davinci-003 in a small scale human study (the five authors of the paper rated model outputs), despite the Alpaca 7B model being much smaller than text-davinci-003. Read the blog post for details.\n\nBlog post: https://crfm.stanford.edu/2023/03/13/alpaca.html\nDemo: https://crfm.stanford.edu/alpaca/\nCode: https://github.com/tatsu-lab/stanford_alpaca", "score": 366}, {"text": "[D] Does anyone have a pdf of Hinton\u2019s talk \u201cAetherial Symbols\u201d?. This talk got referenced in something I was reading, and I was really interested in checking it out, but the links all seem to this [https://drive.google.com/file/d/0B8i61jl8OE3XdHRCSkV1VFNqTWc/view](https://drive.google.com/file/d/0B8i61jl8OE3XdHRCSkV1VFNqTWc/view), which is no longer publicly accessible. I was wondering if anyone had a copy somewhere", "score": 2}, {"text": "[P] ControlNetInpaint: No extra training and you can use \ud83d\udcddtext +\ud83c\udf0cimage + \ud83d\ude37mask to generate new images.. Hi! Here's an **open-source implementation** I released today for masked ControlNet synthesis, where you can specify the region that will be synthesised using a mask. The content of the synthesised region is controlled via textual and visual guidance as shown in the README.\n\n[https://github.com/mikonvergence/ControlNetInpaint](https://github.com/mikonvergence/ControlNetInpaint)\n\nHere's an example with a prompt of ***\"a red panda sitting on a bench\"*****:**\n\nhttps://preview.redd.it/4vxsg9sc0lna1.png?width=1860&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9776369a86043f9420ec8771dd6f9d22308e521c", "score": 85}, {"text": "[N] FastKafka - free open source python lib for building Kafka-based services. We were searching for something like FastAPI for Kafka-based service we were developing, but couldn\u2019t find anything similar. So we shamelessly made one by reusing beloved paradigms from FastAPI and we shamelessly named it FastKafka. The point was to set the expectations right - you get pretty much what you would expect: function decorators for consumers and producers with type hints specifying Pydantic classes for JSON encoding/decoding, automatic message routing to Kafka brokers and documentation generation.\n\nPlease take a look and tell us how to make it better. Our goal is to make using it as easy as possible for someone with experience with FastAPI.\n\nhttps://github.com/airtai/fastkafka", "score": 2}, {"text": "[D]Query on the uniqueness of GPT-based chatbots. I have this question bugging me, and I'm a noob to this. So, if ChatGPT and the likes are all LLMs, built on GPT, and are trained with the same data like from Github, Wikipedia and such, won't they be giving more or less the same answer if each is separately asked the same question?", "score": 3}, {"text": "[P] Build a Question Answer system/chat bot trained on documentation.. Hi everyone! I'm working on a side project for my company where the goal is to train an ML model on the company's documentation. We should then be able to ask it any question based on the docs and it should generate a concise response( something like what chatgpt does). How can I achieve this? \nThanks you in advance :)", "score": 11}, {"text": "[D] Query regarding regression.  I performed linear regression on this dataset - [https://www.kaggle.com/datasets/yasserh/uber-fares-dataset](https://www.kaggle.com/datasets/yasserh/uber-fares-dataset) \n\nBut I have some questions / concerns -\n\n1. Some of the top solutions have derived the time (month number, day of the week \\[mon = 1,...\\]) but while doing regression they didn't do one hot encoding for that even though it is a categorical data.\n2. They have also used year in the regression but I fail to see how that will be relevant to the model\n3. Now I made the following attributes  \nweekday check (binary field) - 1 for weekday, 0 for weekend  \ntime of the day - based on the pickup time I divided the data in 3 parts 00;00 - 08:00 - morning, 08:00 - 16:00 - afternoon, 16:00 - 24:00 - night  \nquarter - divided the months in quarters - Q1 from Jan to March .....\n\nThe problem I'm facing is how to do correlation between the variables since some variables are continuous, some are binary and some are converted after one hot encoding.\n\nAlso, after doing regression I'm getting really low P-values. Like for example =  \nfor distance travelled - P val is = 0 (straight up 0)  \nfor others as well it is E-40, E-120, stuff like that\n\nIs this right ? cause such P values are really too good to be true.\n\nThank you so much !!", "score": 1}, {"text": "[D] ChatGPT without text limits.. One of the biggest limitations of large language models is the text limit. This limits their use cases and prohibits more ambitious prompts.\n\nThis was recently resolved by researchers at Google Brain in Alberta, Canada. In their recent paper they describe a new method of using associative memory which removes the text limit and they also prove that some large language models are universal Turing machines.\n\nThis will pave the way for entire novels being shared with large language models, personal genomes, etc.\n\nThe paper talks about the use of \"associative memory\" which is also known as content-addressable memory (CAM). This type of memory allows the system to retrieve data based on its content rather than its location. Unlike traditional memory systems that use specific memory addresses to access data, associative memory uses CAM to find data based on a pattern or keyword.\n\nPresumably, this will open up a new market for associative memory since I would happily pay some extra money for content to be permanently stored in associative memory and to remove the text limit. This will also drive down the price of associative memory if millions of people are willing to pay a monthly fee for storage and the removal of prompt text limits.\n\nThe paper does point that there are still problems with conditional statements that confuse the large language models. However, I believe this can be resolved with semantic graphs. This would involve collecting data from various sources and using natural language processing techniques to extract entities and relationships from the text. Once the graph is constructed, it could be integrated into the language model in a variety of ways. One approach is to use the graph as an external memory, similar to the approach taken in the paper. The graph can be encoded as a set of key-value pairs and used to augment the model's attention mechanism during inference. The attention mechanism can then focus on relevant nodes in the graph when generating outputs.\n\nAnother potential approach is to incorporate the graph into the model's architecture itself. For example, the graph can be used to inform the initialization of the model's parameters or to guide the attention mechanism during training. This could help the model learn to reason about complex concepts and relationships more effectively, potentially leading to better performance on tasks that require this kind of reasoning.\n\nThe use of knowledge graphs can also help ground truth large language models and reduce hallucinations.\n\nI'm curious to read your thoughts.", "score": 60}, {"text": "[D] ICML 2023 Paper Reviews. ICML 2023 paper reviews are supposed to be released soon. According to the [website](https://icml.cc/Conferences/2023/Dates), they should be released on March 13 (anywhere on earth). I thought to create a discussion thread for us to discuss any issue/complain/celebration or anything else.\n\nThere is so much noise in the reviews every year. Some good work that the authors are proud of might get a low score because of the noisy system, given that ICML is growing so large these years. We should keep in mind that the work is still valuable no matter what the score is.\n\nAccording to the Program Chair's [tweet](https://twitter.com/kchonyc/status/1635126401807585280), it seems that only \\~91% of the reviews are submitted. Hopefully it will not delay the release of the reviews and the start of the rebuttal.", "score": 144}, {"text": "[P] We are building a curated list of awesome curated list closely related to machine learning, looking for contributions.. Hey r/MachineLearning,\n\nWe are collecting a hand-crafted curated list of awesome curated lists closely related to machine learning.\n\nHere is the link to the Github repo: https://github.com/zhimin-z/awesome-awesome-machine-learning\n\nDo any lists need to be included from your perspective? Please let me know, or feel free to submit a pull request.\n\nThe motivation underlying this project is that so many awesome lists regarding machine learning exist on GitHub. But, gradually, it adds a mental burden to memorize where to look for when the ML world is progressing faster and faster these days.\n\nThus, there the project comes, as a unification to sew together all awesome lists closely related to machine learning.", "score": 0}, {"text": "[R] Training Small Diffusion Model. Does anyone have experience training a small diffusion model conditioned on text captions from scratch on 64x64 images or possibly even smaller? \n\nI would like to run it only on images of text to see if it is able to render text. How long would this potentially take if I ran it on 1-2 GPUs? Is this something that\u2019s even possible?", "score": 6}]